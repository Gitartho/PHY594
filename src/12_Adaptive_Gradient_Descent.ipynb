{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Gradient Descent Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 11_Gradient_Descent.ipynb](./11_Gradient_Descent.ipynb)\n",
    "- Gradient Descent: Derived a general framework for optimizing functions of many parameters\n",
    "\n",
    "## Today\n",
    "- Improvements and adaptive methods (step size variation)\n",
    "\n",
    "\n",
    "We learned how a general convex function:\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^{\\top} \\mathsf{A}\\, \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathsf{A} \\in \\mathbb{R}^{M \\times M}$ is a positive semi-definite matrix, can be minimized in an iterative fashion by making steps *downhill* via **gradient descent**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{i+1} \\leftarrow \\mathbf{w}_i - \\eta \\nabla_w f(\\mathbf{w}_i).\n",
    "\\end{equation}\n",
    "\n",
    "Today we will investigate some simple improvements to the gradient descent algorithm for the case $M=2$ that we can easily visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w,A):\n",
    "    return (1/2) * w.T @ A @ w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of this function as a quadratic bowl whose curvature is specified by the value of $A$.\n",
    "\n",
    "It always has a minimum at $f(\\mathbf{w}^*)=0$ given by $\\mathbf{w}^* = (0, 0)^{\\sf T}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our plotting functions for visualization of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_function(grid_1d, func, contours=50, log_contours=False, exact=[0,0]):\n",
    "    '''Make a contour plot over the region described by grid_1d for function func.'''\n",
    "    \n",
    "    # make the 2D grid\n",
    "    X,Y = np.meshgrid(grid_1d, grid_1d, indexing='xy')\n",
    "    Z = np.zeros_like(X)\n",
    "    \n",
    "    # numpy bonus exercise: can you think of a way to vectorize the following for-loop?\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X.T)):\n",
    "            Z[i, j] = func(np.array((X[i, j], Y[i, j])))  # compute function values\n",
    "    \n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    \n",
    "    if not log_contours:\n",
    "        ax.contour(X, Y, Z, contours, cmap='Spectral_r')\n",
    "    else:\n",
    "        ax.contour(X, Y, Z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap='Spectral_r')\n",
    "        \n",
    "    ax.plot(*exact, '*', color='black')\n",
    "\n",
    "    ax.set_xlabel(r'$w_0$')\n",
    "    ax.set_ylabel(r'$w_1$')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    ax3d = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    \n",
    "    if log_contours:\n",
    "        Z = np.log(Z)\n",
    "        label = r'$\\ln f(\\mathbf{w}$'\n",
    "    else:\n",
    "        label = r'$f(\\mathbf{w})$'\n",
    "        \n",
    "    surf = ax3d.plot_surface(X,Y,Z, rstride=1, cstride=1, cmap='Spectral_r', \n",
    "                       linewidth=0, antialiased=True, rasterized=True)\n",
    "    \n",
    "    ax3d.plot([exact[0]], [exact[0]], [func(np.array(exact))], marker='*', ms=6, linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "         \n",
    "    ax3d.set_xlabel(r'$w_0$',labelpad=8)\n",
    "    ax3d.set_ylabel(r'$w_1$',labelpad=8)\n",
    "    ax3d.set_zlabel(label,labelpad=8);\n",
    "    \n",
    "    return fig,ax,ax3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ml4s.random_psd_matrix([2,2])\n",
    "fig,ax,ax3d = plot_function(np.linspace(-5,5,100),lambda x: f(x,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autodiff for derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp # jax has it's own accelerated version of numpy\n",
    "from jax import grad\n",
    "\n",
    "df_dw = grad(f,argnums=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Gradient Descent\n",
    "\n",
    "Now that we know how to take gradients using `jax` we are ready to code up our algorithm.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{i+1} \\leftarrow \\mathbf{w}_i  - \\eta \\nabla_w f(\\mathbf{w}_i) \\ .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "A = ml4s.random_psd_matrix([2,2], seed=0)\n",
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 100), lambda x: f(x, A))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5\n",
    "w = np.array([2.5,-4.0])\n",
    "num_iter = 20\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # we keep a copy of the previous version for plotting\n",
    "    w_old = np.copy(w)\n",
    "    \n",
    "    # perform the GD update\n",
    "    w += -η*df_dw(w, A)\n",
    "    \n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1) \n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [f(w_old,A),f(w,A)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {f(w,A):.6f}$', fontsize=14);\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent with Momentum\n",
    "\n",
    "One problem that arises with the GD algorithm is that retains no **memory** about where it came from and this can lead to problems when there is a rough and/or shallow energy landscape. In physics, this would be equivalent to a ball rolling down a hill that is completely overdamped, i.e. it has no kinetic energy (momentum) to climb out of minima.  \n",
    "\n",
    "This will also allow us to prevent large swings due to local curvature, and will become even more important when our cost functions become high dimensional and we will rely on computing the gradient over only a subset of data (*stochastic gradient descent*).  We modify our above algorithm to include a *memory* or *momentum* term:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_i &\\leftarrow \\gamma \\mathbf{v}_{i-1} + \\eta \\nabla_w f(\\mathbf{w}_i) \\\\\n",
    "\\mathbf{w}_{i+1} &\\leftarrow \\mathbf{w}_i - \\mathbf{v}_{i} \n",
    "\\end{align}\n",
    "\n",
    "where we have introduced a new momentum **hyperparamter** $0 \\le \\gamma < 1$.  For $\\gamma = 0$ we recover ordinary gradient descent, increasing $\\gamma$ increases the information retained about previous steps.  In practice, we often use $\\gamma \\approx 0.9$ which gives us a memory of approximately 10 iterations.  In the literature, you will see this method called *gradient descent with classical momentum* or CM.\n",
    "\n",
    "Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.uniform(low=-5,high=5,size=2)\n",
    "η = 0.5\n",
    "γ = 0.9\n",
    "print(f'f(w) = {f(w,A):.3f}')\n",
    "v = np.zeros(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = γ*v + η*df_dw(w, A)\n",
    "w -= v\n",
    "print(f'f(w) = {f(w,A):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 100), lambda x: f(x, A))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5\n",
    "γ = 0.75\n",
    "num_iter = 20\n",
    "\n",
    "w = np.array([2.5,-4.0])\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "v = np.zeros(2)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    # keep a copy for plotting\n",
    "    w_old = np.copy(w)\n",
    "    \n",
    "    # perform the CM update\n",
    "    v = γ*v + η*df_dw(w, A)\n",
    "    w -= v\n",
    "    \n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1) \n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [f(w_old,A),f(w,A)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {f(w,A):.6f}$', fontsize=14);\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "    <strong>Note:</strong> due to the momentum term, this method is not strictly <em>downhill</em> anymore!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Methods\n",
    "\n",
    "There is a zoo of deferent numerical optimization algorithms.  Check out: https://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "A commonly used variant of CM is the **Nesterov Accelerated Gradient** which is a simple modification of CM that computes the gradient not at $\\mathbf{w}_i$  but at the position that momentum would carry it to at the next time step:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{v}_i &\\leftarrow \\gamma \\mathbf{v}_{i-1} + \\eta \\nabla_w f(\\mathbf{w}_i - \\gamma \\mathbf{v}_{i-1}) \\\\\n",
    "\\mathbf{w}_{i+1} &\\leftarrow \\mathbf{w}_i - \\mathbf{v}_{i} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 100), lambda x: f(x, A))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5\n",
    "γ = 0.9\n",
    "num_iter = 20\n",
    "\n",
    "w = np.array([2.5,-4.0])\n",
    "#w = np.random.uniform(low=-5,high=5,size=2)\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "v = np.zeros(2)\n",
    "\n",
    "for i in range(num_iter):\n",
    "    \n",
    "    # keep a copy for plotting\n",
    "    w_old = np.copy(w)\n",
    "    \n",
    "    # perform the NAG update\n",
    "    v = γ*v + η*df_dw(w-γ*v, A)\n",
    "    w -= v\n",
    "    \n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1) \n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [f(w_old,A),f(w,A)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {f(w,A):.6f}$', fontsize=14);\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapative Methods\n",
    "\n",
    "One of the most important things in practice to improve optimization performance is to change (adapt) the learning rate as a function of time (our index $i$).  This can be done by hand (using a learning schedule) or via algorithms such as **ADAM** which keeps a running average of the first and second moment of the gradient and uses these to update the learning rate for different parameters.  You should watch [Andrew Ng's video on the subject](https://www.youtube.com/watch?v=JXQT_vxqwIs).  I quote the final update scheme here.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{g}_i &= \\nabla_w f(\\mathbf{w}_i) \\\\\n",
    "\\mathbf{m}_i &= \\beta_1 \\mathbf{m}_{i-1} + (1-\\beta_1) \\mathbf{g}_i \\\\\n",
    "\\mathbf{v}_i &= \\beta_2 \\mathbf{v}_{i-1} +(1-\\beta_2)\\mathbf{g}_i^2  \\\\\n",
    "\\hat{\\mathbf{m}}_i &= \\frac{\\mathbf{m}_i}{1-(\\beta_1)^i} \\\\\n",
    "\\hat{\\mathbf{v}}_i &= \\frac{\\mathbf{v}_i}{1-(\\beta_2)^i}  \\\\\n",
    "\\mathbf{w}_{i+1} &=\\mathbf{w}_i - \\eta \\frac{\\hat{\\mathbf{m}}_i}{\\sqrt{\\hat{\\mathbf{v}}_i} +\\epsilon}, \\nonumber \n",
    "\\end{align}\n",
    "\n",
    "where $\\beta_1$ and $\\beta_2$ set the memory lifetime of the first and second moment.  We typically take:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_1 &= 0.9 \\\\\n",
    "\\beta_2 &= 0.999 \\\\\n",
    "\\eta  &= 10^{-3} \\\\\n",
    "\\epsilon &= 10^{-8} \\ . \n",
    "\\end{align}\n",
    "\n",
    "However, for simple functions, a larger starting learning rate such as $\\eta = 10^{-1}$ is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β1 = 0.9\n",
    "β2 = 0.999\n",
    "ϵ = 1.0E-8\n",
    "η = 1.0E-1\n",
    "γ = 0.9\n",
    "\n",
    "num_iter = 1000\n",
    "\n",
    "w = np.random.uniform(low=-5,high=5,size=2)\n",
    "\n",
    "w_traj = np.zeros([num_iter,2])\n",
    "w_traj[0,:] = w\n",
    "\n",
    "m = np.zeros(2)\n",
    "v = np.zeros(2)\n",
    "\n",
    "for i in range(1,num_iter):\n",
    "    \n",
    "    g = np.array(df_dw(w,A))\n",
    "    m = β1*m + (1-β1)*g\n",
    "    v = β2*v + (1-β2)*g*g\n",
    "    \n",
    "    m̂ = m/(1-β1**i)\n",
    "    v̂ = v/(1-β2**i)\n",
    "\n",
    "    w = w - η*np.divide(m̂,np.sqrt(v̂) + ϵ)\n",
    "    w_traj[i,:] = w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot after as we are taking more steps (it would be too slow otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(fig,ax,ax3d,w_traj,func,log_contours=False):\n",
    "    '''Plot the trajectory of a minimization.'''\n",
    "    \n",
    "    num_iter = w_traj.shape[0]\n",
    "    f_traj = np.array([func(w_traj[i,:]) for i in range(num_iter)])\n",
    "    \n",
    "    ax.plot(w_traj[0,0],w_traj[0,1], 'o', color='k', ms=6)    \n",
    "    ax.plot(w_traj[:,0],w_traj[:,1], '.', color='k', ms=1)  \n",
    "    \n",
    "    if log_contours:\n",
    "        f_traj = np.log(f_traj)\n",
    "        \n",
    "    ax3d.plot([w_traj[0,0]], [w_traj[0,1]], [f_traj[0]], marker='o', ms=6, linestyle='-', color='k',lw=1, zorder=100)\n",
    "    ax3d.plot(w_traj[:,0], w_traj[:,1], f_traj, marker='.', ms=1, linestyle='-', color='k',lw=1, zorder=100)\n",
    "    \n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {func(w):.6f}$', fontsize=14);\n",
    "    \n",
    "    return fig,ax,ax3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 100), lambda x: f(x, A))\n",
    "fig, ax, ax3d = plot_trajectory(fig,ax,ax3d,w_traj,lambda x: f(x, A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a nice comparison of methods due to [Alec Radford](https://twitter.com/alecrad).\n",
    "\n",
    "<img src=\"https://ruder.io/content/images/2016/09/contours_evaluation_optimizers.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
