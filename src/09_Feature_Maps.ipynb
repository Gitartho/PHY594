{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Feature Maps for Non-Linear Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Time\n",
    "\n",
    "#### [Notebook Link: 08_Linear_Regression_Exercise.ipynb](./08_Linear_Regression_Exercise.ipynb)\n",
    "\n",
    "- Cost functions and formulating a machine learning task as an optimization problem\n",
    "- Multidimensional linear regression \n",
    "\n",
    "### Today\n",
    "\n",
    "- Learn how linear regression can learn non-linear functions using feature maps.\n",
    "- Understanding model complexity and the bias-variance tradeoff\n",
    "\n",
    "We can generalize everything we learned about linear regression to non-linear models composed of a linear combination of non-linear parts, *i.e.* feature maps.  Let us change our notation slightly (as we would like to be fully general).  As before, our goal is to predict a scalar *target* $y$ as a function of $\\vec{x}$ given a dataset of pairs $\\mathcal{D} = \\{(\\vec{x}^{(n)},y^{(n)})\\}_{n=1}^N$.  Here the $\\vec{x}^{(n)}$ are inputs and the $y^{(n)}$ are targets or observations.  We now let our model be more general:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "F(\\vec{x},\\vec{w}) = \\vec{\\varphi}(\\vec{x}) \\cdot \\vec{w} }\n",
    "\\end{equation}\n",
    "\n",
    "which is **linear** in the *weights* (here we have incorporated a potential bias as $w_0$): \n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w} = \\left( \\begin{array}{c}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_{M-1}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "but can be **non-linear** in $\\vec{x}$ depending on the basis functions features (think pre-processing layer):\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\varphi}(\\vec{x}) = \\left(\n",
    "\\varphi_0(\\vec{x}), \n",
    "\\varphi_1(\\vec{x}),\n",
    "\\dots,\n",
    "\\varphi_{M-1}(\\vec{x})\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\phi_j(\\vec{x}) : \\mathbb{R}^D \\mapsto \\mathbb{R}, \\forall j \\in [0,\\dots,M-1]$.\n",
    "\n",
    "### Examples of Basis Functions\n",
    "1. **Identity Transformation:** $\\varphi_j(\\vec{x}) = x_j$; this is just the linear regression we have already seen. Here $M = D$.\n",
    "2. **Polynomial Decomposition:** $\\varphi_j(\\vec{x}) = |\\vec{x}|^j$; note that the basis functions are global, they behave non-trivially on the entire domain of $x$.\n",
    "3. **Sigmoid Basis:** $\\varphi_j(\\vec{x}) = \\sigma((x_j-\\mu_j)/s)$ where $\\sigma(z) = 1/(1+\\mathrm{e}^{-z})$; note that this can affect a local region of the domain non-trivially.\n",
    "4. **Fourier  Basis:** $\\varphi_j(\\vec{x}) = \\mathrm{e}^{i 2\\pi x_j j}$; action is local in the frequency domain.\n",
    "\n",
    "#### Consider a 3rd order polynomial in the scalar $x$ (i.e. $D = 1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [1,4,1]\n",
    "labels = [[r'$x$'],['$1$','$x$','$x^2$','$x^3$'],[r'$F(x,\\vec{w})$']]\n",
    "ml4s.draw_network(N,node_labels=labels, weights=[['' for i in range(N[1])],[f'$w_{i}$' for i in range(N[1])]], biases=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our previous derivations go through unchanged provided we replace our design matrix $\\mathbf{X}$ with a new feature matrix $\\mathbf{\\Phi}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi} = \\left( \\begin{array}{cccc}\n",
    "        \\varphi_0(\\vec{x}^{(1)}) & \\varphi_1(\\vec{x}^{(1)}) & \\cdots & \\varphi_{M-1}(\\vec{x}^{(1)}) \\\\\n",
    "        \\varphi_0(\\vec{x}^{(2)}) & \\varphi_1(\\vec{x}^{(2)}) & \\cdots & \\varphi_{M-1}(\\vec{x}^{(2)}) \\\\\n",
    "\\vdots        &      \\vdots    & \\ddots & \\vdots \\\\\n",
    "\\varphi_0(\\vec{x}^{(N)}) & \\varphi_1(\\vec{x}^{(1)}) & \\cdots & \\varphi_{M-1}(\\vec{x}^{(N)}) \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "which, when minimizing the squared error costs across the entire dataset:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\lvert \\lvert F^{(n)}(\\vec{x}^{(n)},\\vec{w}) - y^{(n)} \\rvert \\rvert^2\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "yields the optimal parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}^\\ast = \\left(\\mathbf{\\Phi}^{\\sf T} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\sf T} \\mathbf{y}.\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{equation}\n",
    "\\mathbf{y} = \\left(\n",
    "\\begin{array}{c}\n",
    "y^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(N)}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "ss the vector of targets (corresponding to each sample in the dataset $\\mathcal{D}$).\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Load data from disk `../data/poly_regression.dat`\n",
    "\n",
    "\n",
    "<!--\n",
    "x = np.linspace(0,1,10)\n",
    "header = f\"{'x':>13s}\\t{'y':>15s}\"\n",
    "data_out = np.column_stack([x,np.sin(2*np.pi*x)+np.random.normal(loc=0,scale=0.15,size=x.size)])\n",
    "np.savetxt('../data/poly_regression.dat', data_out,fmt='% 15.8e', header=header, delimiter='\\t')\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/poly_regression.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/poly_regression.dat',unpack=True)\n",
    "plt.plot(x,y, 'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What should our model look like? \n",
    "\n",
    "We can guess by looking at the number of zero crossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_order = 3\n",
    "Φ = np.zeros([len(x),poly_order+1])\n",
    "for j in range(Φ.shape[1]):\n",
    "    Φ[:,j] = x**j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_opt = np.dot(np.dot(np.linalg.inv(np.dot(Φ.T,Φ)),Φ.T),y)\n",
    "C_opt = 0.5*np.average((np.dot(Φ,W_opt)-y)**2)\n",
    "\n",
    "print(f'W_opt = {W_opt}')\n",
    "print(f'C_opt = {C_opt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can compare this with the `np.polyfit` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit(x,y,poly_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "Remember: <tt>np.polyfit()</tt> reverses the order of the coeffecients.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data with the optimized fit\n",
    "\n",
    "Again we can now evaluate our data at all points (*interpolation*) inside our fitting domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o', label='data')\n",
    "\n",
    "x_fit = np.linspace(np.min(x),np.max(x),100)\n",
    "Φ_fit = np.zeros([len(x_fit),poly_order+1])\n",
    "\n",
    "# we use the fact that x^0 = 1\n",
    "for j in range(Φ.shape[1]):\n",
    "    Φ_fit[:,j] = x_fit**j\n",
    "\n",
    "plt.plot(x_fit,Φ_fit @ W_opt,'-', color=colors[0], label='fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
