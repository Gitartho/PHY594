{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "\n",
    "# t-SNE & Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "π = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 31_Autoencoders.ipynb](31_Autoencoders.ipynb)\n",
    "\n",
    "- Introduced unsupervised learning, or discovering patterns and structure in unlabelled data.\n",
    "- Discussed principal component analysis, which allowed for dimensional reduction by projecting onto components which capture the largest variance (signal) in the data.\n",
    "- Studied MNIST data set, and identified some clusters, but without prior labelling, separation into clusters would be problematic.\n",
    "\n",
    "## Today\n",
    "- Curse of dimensionality.\n",
    "- Clustering: group unlabeled data into clusters according to some similarity metric.\n",
    "    - t-SNE: preserving local structures when projecting to low dimensions.\n",
    "    - Identification of latent variables.\n",
    "    - Algorithms provided by `sklearn`\n",
    "\n",
    "\n",
    "## Curse of Dimensionality\n",
    "\n",
    "In high dimensions, everthing lives on the edge!\n",
    "\n",
    "Consider a space with $D \\gg 1$ and generate uniform random numbers inside a hypercube of side length $d$, i.e. $x \\in \\mathcal{U}_{\\Omega}$ where $\\Omega = d^D$.  Now let's determine how many of them fall within the hypersphere of diameter $d$ centered at the origin. Let's visualize for $D = 3$.\n",
    "\n",
    "<!--\n",
    "N = 2000\n",
    "x = np.random.normal(loc=[0,0],scale=[1,0.4], size=[N,2])\n",
    "\n",
    "θ = np.radians(35)\n",
    "R = np.array([[np.cos(θ), -np.sin(θ)], [np.sin(θ), np.sin(θ)]])\n",
    "for i in range(N):\n",
    "    x[i] = R @ x[i]\n",
    "np.savetxt('../data/scatter_2d_pca.dat',x)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10**4\n",
    "d = 1.0\n",
    "D = 3\n",
    "C = np.random.uniform(low=-d/2, high=d/2,size=(N,D))\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=plt.figaspect(1)*2)\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# we find the points inside the hypersphere\n",
    "S = np.where(np.sum(C**2,axis=1) < (d/2)**2)[0]\n",
    "\n",
    "ax.scatter(C[S,0], C[S,1], C[S,2], c=colors[0], marker='.', rasterized=True)\n",
    "ax.scatter(C[~S,0], C[~S,1], C[~S,2], c=colors[-2], marker='.', alpha=0.2, rasterized=True)\n",
    "\n",
    "ax.set_xlabel('x',labelpad=14)\n",
    "ax.set_ylabel('y',labelpad=14)\n",
    "ax.set_zlabel('z',)\n",
    "\n",
    "ax.set_xlim(-0.6,0.6)\n",
    "ax.set_ylim(-0.6,0.6)\n",
    "ax.set_zlim(-0.6,0.6)\n",
    "\n",
    "print(f'Sphere/Cube = {S.shape[0]/C.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as the dimension increases, this fraction is reduced rapidly and is given by the volume ratio:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{V_{\\mathcal{S}}}{\\Omega} = \\frac{\\frac{\\pi^{D/2} (d/2)^D}{\\Gamma(D/2 + 1)}}{d^D} \n",
    "%\\approx \\frac{e^{-\\frac{1}{2} D \\left(\\log \\left(\\frac{2 D}{\\pi }\\right)-1\\right)}}{\\sqrt{\\pi } \\sqrt{D}} \n",
    "\\sim \\left(\\frac{1}{2}\\right)^D\\, .\n",
    "\\end{equation}\n",
    "\n",
    "This tells us that for uniformly distributed data, the number of points found inside the hypersphere vanishes exponentially fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "N = 10**6\n",
    "D = np.linspace(1,20,20).astype(int)\n",
    "\n",
    "ratio = []\n",
    "for cD in D:\n",
    "    C = np.random.uniform(low=-d/2, high=d/2,size=(N,cD))\n",
    "    S = np.where(np.sum(C**2,axis=1) < (d/2)**2)[0]\n",
    "    ratio.append(S.shape[0]/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import gamma as Γ\n",
    "Vratio = np.exp(-0.5*D*(-1.0+np.log(2.0*D/π)))/(π*np.sqrt(D))\n",
    "plt.semilogy(D,ratio,'o', mfc='None', color=colors[0], label='Data')\n",
    "plt.semilogy(D,π**(D/2)*(d/2)**D/Γ(D/2+1),'-', color=colors[0], label=r'$V_\\mathcal{S}/\\Omega$')\n",
    "plt.semilogy(D,0.5**D,'-', color=colors[3], label=r'$2^{-D}$', zorder=-1)\n",
    "plt.xlabel('Spatial Dimension  (D)')\n",
    "plt.ylabel('Edge to Bulk Ratio')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data is non-Uniform\n",
    "\n",
    "As we saw last time, we were able to understand most of the features of our $D=784$ MNIST data set with only a few principal components.  This is due to the large amount of *structure* in real-world data. There is a famous example called the *swiss roll* which demonstrates that the minimum number of parameters required to parameterize the data is equal to its intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "x1 = np.linspace(0,2,N)\n",
    "x2 = np.random.uniform(low=0,high=4*π, size=N)\n",
    "\n",
    "x = np.column_stack((x1,x2*np.sin(x2),x2*np.cos(x2)))\n",
    "\n",
    "# extract colors corresponding to angle mod 2π\n",
    "c = np.array(colors)[np.array(np.fmod(x2,2*π) // (2*π/10)).astype(int)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "ax = [fig.add_subplot(1, 2, 1, projection='3d'),fig.add_subplot(1, 2, 2)]\n",
    "\n",
    "ax[0].scatter(x[:,0],x[:,1],x[:,2], c=c, marker='.')\n",
    "\n",
    "ax[0].set_xlabel('x',labelpad=14)\n",
    "ax[0].set_ylabel('y',labelpad=14)\n",
    "ax[0].set_zlabel('z')\n",
    "\n",
    "ax[0].view_init(15, 30)\n",
    "\n",
    "ax[1].scatter(x1,x2,c=c, marker='.')\n",
    "ax[1].set_xlabel(r'$x_1$')\n",
    "ax[1].set_ylabel(r'$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can PCA determine the intrinsic dimensionality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# perform the PCA\n",
    "model = PCA(n_components=2)\n",
    "XPCA = model.fit_transform(x)\n",
    "\n",
    "# store the results\n",
    "λ = model.explained_variance_\n",
    "PCAj = model.explained_variance_ratio_\n",
    "V = model.components_\n",
    "\n",
    "# project onto the first 2 principal components\n",
    "pX = x @ V.T\n",
    "\n",
    "# visualize the results\n",
    "\n",
    "plt.scatter(pX[:,0],pX[:,1], c=c, marker='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple projection onto the y-z plane (the two dimensions with greatest variance) so PCA is doing exaclty what it is supposed to, but it is not enough to fully identify patterns in the data.  This is related to the crowding problem that can arise when we try to preserve distances upon projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "square = np.array([[-0.5,-0.5],[-0.5,0.5],[0.5,-0.5],[0.5,0.5]])\n",
    "ax[0].scatter(square[:,0],square[:,1], c=colors[:8:2], s=400)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([]);\n",
    "\n",
    "proj_square = np.array([[-0.5,0.500],[-0.5+0.01,0.5],[0.5,0.5],[0.5+0.01,0.5]])\n",
    "ax[1].scatter(proj_square[:,0],proj_square[:,1], c=colors[:8:2], s=400)\n",
    "\n",
    "ax[1].axhline(y=0, ls='--', color='k')\n",
    "\n",
    "proj_square = np.array([[-np.sqrt(2)/2,-0.5],[-0.005,-0.5],[0.005,-0.5],[np.sqrt(2)/2,-0.5]])\n",
    "ax[1].scatter(proj_square[:,0],proj_square[:,1], c=colors[:8:2], s=400)\n",
    "\n",
    "ax[1].set_ylim(-1,1)\n",
    "ax[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem arises as we are trying to satisfy too many constraints (i.e. all distances preserved).  This can be addressed by weakening the constraint and prioritizing preserving distances that are **close** in the high-dimensional space.  \n",
    "\n",
    "## t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "This is an extremely powerful algorithm developped in 2008:\n",
    "- [L.J.P. van der Maaten and G.E. Hinton, *Visualizing High-Dimensional Data Using t-SNE*, Journal of Machine Learning Research **9**, 2579 (2008).](https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf)\n",
    "\n",
    "The goal is to reproduce distances from the higher dimensional space inside the latent space as closely as possible, realizing this is not a perfectly possible (hence stochastic).\n",
    "\n",
    "We can define a cost function $\\mathcal{C}$ of the relative separation of points $\\|\\boldsymbol{x}_i-\\boldsymbol{x}_j\\|$ in the $D$ dimensional space and $\\|\\boldsymbol{y}_i-\\boldsymbol{y}_j\\|$ in the latent lower dimensional space.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{C} = \\sum_{i \\ne j} F(\\|\\boldsymbol{x}_i-\\boldsymbol{x}_j\\|,\\|\\boldsymbol{y}_i-\\boldsymbol{y}_j\\|)\\, ,\n",
    "\\end{equation}\n",
    "\n",
    "that can be minimized using our tools like gradient descent. We want to choose a function that imposes a penalty (repulsion) if points in the low-dimensional space are closer than in the higher dimensional one, and otherwise attracts them.  In *Stochastic Neighbor Embedding*, this is accomplished by defining probability distributions associated with the neighborhood of each data point in the high-dimensional ($p_{ij}$) and low-dimensional ($q_{ij}$) spaces subject to the normalization condition that $\\sum_{i\\ne j} p_{ij} = \\sum_{i\\ne j} q_{ij} = 1$.\n",
    "\n",
    "We want to pick these distributions such that the probability to pick a pair of points is **larger** if they are close neighbors in the high-dimensional space, and similar in the low-dimenisional one.  Our goal is to make these two probability distributions as close as possible (within the restrictions of embedding to the latent space).   This is a well-studied problem in probability theory and the correct cost function is known as the **Kullback-Leibler divergence**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{C} = KL(p \\| q) = \\sum_{ij} p_{ij} \\log \\frac{p_{ij}}{q_{ij}} \\,  .\n",
    "\\end{equation}\n",
    "\n",
    "Now the only thing left to do is to choose the distributions $p_{ij}$ and $q_{ij}$.  The underlying heuristics for this are explained in the paper above, but a natural choice is a Gaussian distribution in the higher-dimensional space $D$:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{j \\mid i}=\\frac{\\exp \\left(-\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{j}\\right\\|^{2} / 2 \\sigma_{i}^{2}\\right)}{\\sum_{k \\neq i} \\exp \\left(-\\left\\|\\boldsymbol{x}_{i}-\\boldsymbol{x}_{k}\\right\\|^{2} / 2 \\sigma_{i}^{2}\\right)}\n",
    "\\end{equation}\n",
    "\n",
    "where we interpret $p_{j \\mid i}$ as the liklihood that point $\\boldsymbol{x}_j$ is a neighbor of $\\boldsymbol{x}_i$ and thus $p_{i \\mid i} = 0$  These are high-dimensional Gaussian distributions and $\\sigma_i$ are parameters that can be set by fixing the entropy of each data point:\n",
    "\n",
    "\\begin{equation}\n",
    "H(p_i) \\equiv - \\sum_j p_{j \\mid i} \\log_2 p_{i \\mid j}\n",
    "\\end{equation}\n",
    "\n",
    "such that $\\Upsilon = 2^{H(p_i)}$ is equal to the same number (the *perplexity*) for each data point.  This uniquely determines $\\sigma_i$ for each point and ensures that low-density regions have larger $\\sigma_i$ values.  To ensure that each data point has a non-vanishing contribution to the cost function, we define the symmetric distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "p_{ij} = \\frac{p_{j \\mid i} + p_{i \\mid j}}{2N}\\, \n",
    "\\end{equation}\n",
    "\n",
    "which is our final probability.  \n",
    "\n",
    "For our lower-dimensional latent space we choose coordinate $\\boldsymbol{Y} = \\{\\boldsymbol{y}_i\\}_{j=1}^{N}$ where $\\boldsymbol{y}_i \\in \\mathbb{R}^M$ with $M \\ll D$. We could also choose a Gaussian in lower dimensions, but it turns out this has a problem: **the exponential supression at long distances from the Gaussian distribution is too restrictive in the latent space**.  As we don't have as much *space* here, it makes sense to relax the cost for incorrect distances for points that aren't too close in the $D$-dimensional space as these points are not strongly correlated. We thus want to find a distrubition with fatter tails than the Gaussian: **Student's t-distribution**:\n",
    "\n",
    "\\begin{equation}\n",
    "q_{i j}=\\frac{\\left(1+\\left\\|\\boldsymbol{y}_{i}-\\boldsymbol{y}_{j}\\right\\|^{2}\\right)^{-1}}{\\sum_{k \\neq i}\\left(1+\\left\\|\\boldsymbol{y}_{k}-\\boldsymbol{y}_{l}\\right\\|^{2}\\right)^{-1}}\\, .\n",
    "\\end{equation}\n",
    "\n",
    "#### Procedure:\n",
    "\n",
    "1. $ p_{ij}$ are fixed by the $D$-dimensional data set.  Compute for a given perplexity.\n",
    "2. Guess an initial set $\\{\\boldsymbol{y}_j\\}$ in the latent space defined by $M$.\n",
    "3. Compute $q_{ij}$ and the cost $\\mathcal{C}$ given by the KL divergence.\n",
    "4. Perform an update via gradient descent $\\boldsymbol{y}_i \\to \\boldsymbol{y}_i - \\eta \\boldsymbol{\\nabla}_{\\boldsymbol{y}_i} \\mathcal{C}$.\n",
    "5. Iterate steps 3-4 until convergence.\n",
    "\n",
    "Code from https://lvdmaaten.github.io/tsne/ is included as function `ml4s.tsne`.  Let's see how it works!\n",
    "\n",
    "\n",
    "<!--\n",
    "x = np.array([[0,0],[1,1],[4,4]])\n",
    "y = np.array([0,1,50])\n",
    "\n",
    "def p(i,j,x):\n",
    "    σ = 1.0\n",
    "    pji = np.exp(-np.sum((x[i]-x[j])**2)/(2*σ**2))\n",
    "    norm = 0.0\n",
    "    for k in range(x.shape[0]):\n",
    "        if k != i:\n",
    "            norm += np.exp(-np.sum((x[i]-x[k])**2)/(2*σ**2))\n",
    "    pji /= norm\n",
    "    \n",
    "    pij = np.exp(-np.sum((x[j]-x[i])**2)/(2*σ**2))\n",
    "    norm = 0.0\n",
    "    for k in range(x.shape[0]):\n",
    "        if k != j:\n",
    "            norm += np.exp(-np.sum((x[j]-x[k])**2)/(2*σ**2))\n",
    "    pij /= norm\n",
    "\n",
    "    return (pij + pji) / (2*x.shape[0])\n",
    "\n",
    "def q(i,j,y):\n",
    "    qij = 1.0 / (1 + np.sum((y[i]-y[j])**2))\n",
    "    norm = 0.0\n",
    "    for k in range(y.shape[0]):\n",
    "        if k != i:\n",
    "            norm += 1.0 / (1 + np.sum((y[i]-y[k])**2))\n",
    "    return qij/norm\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a D-dimensional data set of Gaussian point clouds\n",
    "D = 100\n",
    "\n",
    "# number of clouds\n",
    "n_clouds = 10 \n",
    "\n",
    "# number of points per cloud\n",
    "N_cloud_points = 40 \n",
    "\n",
    "# total number of pouints\n",
    "N = n_clouds*N_cloud_points # total number of points\n",
    "\n",
    "# cloud size\n",
    "σ = 1.0\n",
    "\n",
    "# create the clusters and labels \n",
    "# they are each distributed about a common point\n",
    "X,y = np.zeros([N,D]), np.zeros([N])\n",
    "for j in range(n_clouds):\n",
    "    \n",
    "    # cluster label\n",
    "    y[j*N_cloud_points:(j+1)*N_cloud_points] = j\n",
    "    \n",
    "    # the center position of the cloud\n",
    "    X̄ = np.random.normal(size=(D)) \n",
    "    \n",
    "    # the cloud points\n",
    "    X[j*N_cloud_points:(j+1)*N_cloud_points,:] = X̄[np.newaxis,:] + σ*np.random.normal(size=(N_cloud_points,D))\n",
    "\n",
    "# Project clouds into 2D and color\n",
    "# compare with PCA\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=(10,4))\n",
    "scatter = ax[0].scatter(X[:,0],X[:,1],c=y,cmap='Spectral_r')\n",
    "\n",
    "legend = ax[0].legend(*scatter.legend_elements(),loc=(2.2,0), title=\"Cloud Label\")\n",
    "ax[0].add_artist(legend)\n",
    "ax[0].set_xlabel(r'$X_1$')\n",
    "ax[0].set_ylabel(r'$X_2$');\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "M = 2\n",
    "model = PCA(n_components=2)\n",
    "XPCA = model.fit_transform(X)\n",
    "\n",
    "# store the results\n",
    "λ = model.explained_variance_\n",
    "PCAj = model.explained_variance_ratio_\n",
    "V = model.components_\n",
    "\n",
    "pX = X @ V.T\n",
    "\n",
    "scatter = ax[1].scatter(pX[:,0],pX[:,1],c=y,cmap='Spectral_r')\n",
    "ax[1].set_xlabel(f'PCA-1 = {PCAj[0]:.2f}')\n",
    "ax[1].set_ylabel(f'PCA-2 = {PCAj[1]:.2f}')\n",
    "fig.subplots_adjust(hspace = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the t-SNE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ml4s.tsne(X, no_dims=2, initial_dims=50, perplexity=5.0, \n",
    "       do_animation=True, animation_skip_steps=10, max_iter=300)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "scatter = ax.scatter(Y[:,0],Y[:,1],c=y,cmap='Spectral_r')\n",
    "\n",
    "legend = ax.legend(*scatter.legend_elements(),loc=(1,0), title=\"Cloud Label\")\n",
    "ax.add_artist(legend)\n",
    "ax.set_xlabel(r'$Y_1$')\n",
    "ax.set_ylabel(r'$Y_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do this for the MNIST Data Set\n",
    "\n",
    "We project onto the first 50 princpal components. See [here](https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca) to understand the relationship between PCA and the singular value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# MNIST exists in many places!\n",
    "mnist = fetch_openml('mnist_784',parser='auto')\n",
    "X = mnist.data\n",
    "y = mnist.target.astype('int')\n",
    "\n",
    "# Do PCA and project onto 50 dimensions\n",
    "X = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X, full_matrices=False)\n",
    "X50 = np.dot(U, np.diag(s))[:,:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-SNE is Slow!\n",
    "\n",
    "The conventional t-SNE algorithm we used above scales as $\\mathrm{O}(N^2)$ where $N$ is the number of data points.  Recent algorithms use the [Fast Fourier transform](https://github.com/KlugerLab/FIt-SNE) and are extermely effecient. We load a cython wrapper around a c++ implementation https://github.com/KlugerLab/pyFIt-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make sure the numpy array is prepared in C-order\n",
    "X50 = X50.copy(order='C')\n",
    "\n",
    "np.random.seed(0)\n",
    "Y = fitsne.FItSNE(X50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "scatter = ax.scatter(Y[:,0],Y[:,1],s=2,c=y,cmap='Spectral_r',edgecolors='None')\n",
    "\n",
    "# produce a legend with the unique colors corresponding to digits\n",
    "legend = ax.legend(*scatter.legend_elements(),loc=(1,0), title=\"MNIST Digits\")\n",
    "ax.add_artist(legend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Algorithms\n",
    "\n",
    "In general there is a zoo of clustering algorithms that can discover structure in unlabelled data.  While they can certainly aid in data visualization, the goal of clutering is to group unlabelled data into distinct classes (clusters) based on some similarity or distance measure.  We usually take this to be the euclidean distance.  \n",
    "\n",
    "Different algorithms have different tradeoffs depending on the type of data under consideration (e.g. overlapping vs. well separated, low vs. high-dimensional data) and have different algorithmic scaling.  Many are included in `sklearn` and here we take the example code from  [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py) to get a flavor for how they work on some toy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import cycle, islice\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# ============\n",
    "# Generate datasets. We choose the size big enough to see the scalability\n",
    "# of the algorithms, but not too big to avoid too long running times\n",
    "# ============\n",
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "no_structure = np.random.rand(n_samples, 2), None\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "\n",
    "# blobs with varied variances\n",
    "varied = datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)\n",
    "\n",
    "# ============\n",
    "# Set up cluster parameters\n",
    "# ============\n",
    "plt.figure(figsize=(9 * 2 + 3, 12.5))\n",
    "plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05,\n",
    "                    hspace=.01)\n",
    "\n",
    "plot_num = 1\n",
    "\n",
    "default_base = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 3,\n",
    "                'min_samples': 20,\n",
    "                'xi': 0.05,\n",
    "                'min_cluster_size': 0.1}\n",
    "\n",
    "datasets = [\n",
    "    (noisy_circles, {'damping': .77, 'preference': -240,\n",
    "                     'quantile': .2, 'n_clusters': 2,\n",
    "                     'min_samples': 20, 'xi': 0.25}),\n",
    "    (noisy_moons, {'damping': .75, 'preference': -220, 'n_clusters': 2}),\n",
    "    (varied, {'eps': .18, 'n_neighbors': 2,\n",
    "              'min_samples': 5, 'xi': 0.035, 'min_cluster_size': .2}),\n",
    "    (aniso, {'eps': .15, 'n_neighbors': 2,\n",
    "             'min_samples': 20, 'xi': 0.1, 'min_cluster_size': .2}),\n",
    "    (blobs, {}),\n",
    "    (no_structure, {})]\n",
    "\n",
    "for i_dataset, (dataset, algo_params) in enumerate(datasets):\n",
    "    # update parameters with dataset-specific values\n",
    "    params = default_base.copy()\n",
    "    params.update(algo_params)\n",
    "\n",
    "    X, y = dataset\n",
    "\n",
    "    # normalize dataset for easier parameter selection\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # estimate bandwidth for mean shift\n",
    "    bandwidth = cluster.estimate_bandwidth(X, quantile=params['quantile'])\n",
    "\n",
    "    # connectivity matrix for structured Ward\n",
    "    connectivity = kneighbors_graph(\n",
    "        X, n_neighbors=params['n_neighbors'], include_self=False)\n",
    "    # make connectivity symmetric\n",
    "    connectivity = 0.5 * (connectivity + connectivity.T)\n",
    "\n",
    "    # ============\n",
    "    # Create cluster objects\n",
    "    # ============\n",
    "    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "    ward = cluster.AgglomerativeClustering(\n",
    "        n_clusters=params['n_clusters'], linkage='ward',\n",
    "        connectivity=connectivity)\n",
    "    spectral = cluster.SpectralClustering(\n",
    "        n_clusters=params['n_clusters'], eigen_solver='arpack',\n",
    "        affinity=\"nearest_neighbors\")\n",
    "    dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "    optics = cluster.OPTICS(min_samples=params['min_samples'],\n",
    "                            xi=params['xi'],\n",
    "                            min_cluster_size=params['min_cluster_size'])\n",
    "    affinity_propagation = cluster.AffinityPropagation(\n",
    "        damping=params['damping'], preference=params['preference'])\n",
    "    average_linkage = cluster.AgglomerativeClustering(\n",
    "        linkage=\"average\", affinity=\"cityblock\",\n",
    "        n_clusters=params['n_clusters'], connectivity=connectivity)\n",
    "    birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "    gmm = mixture.GaussianMixture(\n",
    "        n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "    clustering_algorithms = (\n",
    "        ('MiniBatchKMeans', two_means),\n",
    "        ('AffinityPropagation', affinity_propagation),\n",
    "        ('MeanShift', ms),\n",
    "        ('SpectralClustering', spectral),\n",
    "        ('Ward', ward),\n",
    "        ('AgglomerativeClustering', average_linkage),\n",
    "        ('DBSCAN', dbscan),\n",
    "        ('OPTICS', optics),\n",
    "        ('Birch', birch),\n",
    "        ('GaussianMixture', gmm)\n",
    "    )\n",
    "\n",
    "    for name, algorithm in clustering_algorithms:\n",
    "        t0 = time.time()\n",
    "\n",
    "        # catch warnings related to kneighbors_graph\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the \" +\n",
    "                \"connectivity matrix is [0-9]{1,2}\" +\n",
    "                \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"Graph is not fully connected, spectral embedding\" +\n",
    "                \" may not work as expected.\",\n",
    "                category=UserWarning)\n",
    "            algorithm.fit(X)\n",
    "\n",
    "        t1 = time.time()\n",
    "        if hasattr(algorithm, 'labels_'):\n",
    "            y_pred = algorithm.labels_.astype(int)\n",
    "        else:\n",
    "            y_pred = algorithm.predict(X)\n",
    "\n",
    "        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n",
    "        if i_dataset == 0:\n",
    "            plt.title(name, size=18)\n",
    "        \n",
    "        _colors = np.array(colors[::2])\n",
    "        # add black color for outliers (if any)\n",
    "        _colors = np.append(colors, [\"#000000\"])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=_colors[y_pred])\n",
    "\n",
    "        plt.xlim(-2.5, 2.5)\n",
    "        plt.ylim(-2.5, 2.5)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "        plt.text(.99, .01, ('%.2fs' % (t1 - t0)).lstrip('0'),\n",
    "                 transform=plt.gca().transAxes, size=15,\n",
    "                 horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml4s)",
   "language": "python",
   "name": "ml4s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
