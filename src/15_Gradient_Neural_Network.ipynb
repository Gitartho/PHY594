{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Gradient of a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "import jax.numpy as jnp \n",
    "from jax import grad\n",
    "from jax import random\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "π = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 14_Stochastic_Gradient_Descent.ipynb](./14_Stochastic_Gradient_Descent.ipynb)\n",
    "\n",
    "- Use randomness to effeciently optimize a rough convex function\n",
    "- Minibatches (data subsets) and epochs (all possible minibatches)\n",
    "\n",
    "## Today\n",
    "\n",
    "- Taking the gradient of a neural network.\n",
    "\n",
    "In lecture, we manually determined the gradient of the cost function of a shallow neural network with 2 input neurons and one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml4s.draw_network([2,1], zero_index=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost & Gradient\n",
    "\n",
    "For a data set: $\\mathcal{D} = \\left\\{\\vec{x}^{(n)},y^{(n)}\\right\\}_{n=1}^{N}$, we wrote the cost function as the usual least squared error:\n",
    "\n",
    "\\begin{equation}\n",
    "C\\left(w_{11}^1,w_{21}^1,b_1^1; \\mathcal{D}\\right) = \\frac{1}{2} \\langle \\left(a_1^1 - y\\right)^2 \\rangle\n",
    "\\end{equation}\n",
    "\n",
    "where $\\langle \\dots \\rangle$ indicates an average over $\\mathcal{D}$.  By using the chain rule we found:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial w_{j1}^1} &= \\langle \\left(a_1^1 - y\\right)f^\\prime\\left(z_1^1\\right)x_j  \\rangle \\\\\n",
    "\\frac{\\partial C}{\\partial b_{1}^1} &= \\langle \\left(a_1^1 - y\\right)f^\\prime\\left(z_1^1\\right) \\rangle \\, .\n",
    "\\end{align}\n",
    "\n",
    "Let's assume our non-linearity is a sigmoid:\n",
    "\n",
    "\\begin{equation}\n",
    "f(z) = \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "such that the derivative will be:\n",
    "\\begin{equation}\n",
    "f^\\prime(z) = \\frac{e^{-z}}{\\left(1+e^{-z}\\right)^2}.\n",
    "\\end{equation}\n",
    "\n",
    "I will use `jax.numpy` for these functions as a drop-in replacement for `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def σ(z):\n",
    "    return 1.0/(1.0 + jnp.exp(-z))\n",
    "\n",
    "def σ_prime(z):\n",
    "    return jnp.exp(-z)/(1.0 + jnp.exp(-z))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward\n",
    "\n",
    "The prediction of our network is a sipmle 1-line funciton (we have a shallow network with no hidden layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(w, b, x):\n",
    "    return σ(jnp.dot(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A synthetic data set\n",
    "\n",
    "We will just check the accuracy of our derivative method, so we generate a random (synthetic) data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize = 10\n",
    "x = np.random.uniform(low=-1,high=1,size=[batchsize,2])\n",
    "y = np.random.uniform(low=-2,high=2,size=[batchsize,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Network\n",
    "\n",
    "As we have done in the past, let's start with some random weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.uniform(low=-1,high=1,size=[2])\n",
    "b = np.random.uniform(low=-1,high=1,size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cost function\n",
    "\n",
    "\\begin{equation}\n",
    "C\\left(w_{11}^1,w_{21}^1,b_1^1; \\mathcal{D}\\right) = \\frac{1}{2} \\langle \\left(a_1^1 - y\\right)^2 \\rangle\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = jnp.dot(x, w) + b\n",
    "a1 = σ(z1)\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(w,b,x,y):\n",
    "    a1 = feed_forward(w,b,x)\n",
    "    return 0.5*jnp.average((a1-y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(w,b,x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of the cost function\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial w_{j1}^1} &= \\langle \\left(a_1^1 - y\\right)f^\\prime\\left(z_1^1\\right)x_j  \\rangle \\\\\n",
    "\\frac{\\partial C}{\\partial b_{1}^1} &= \\langle \\left(a_1^1 - y\\right)f^\\prime\\left(z_1^1\\right) \\rangle \\, .\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dC_dw(w,b,x,y):\n",
    "    z1 = jnp.dot(x, w) + b\n",
    "    a1 = σ(z1)\n",
    "    fp = σ_prime(z1)\n",
    "    return jnp.dot((a1-y)*fp, x) / x.shape[0]\n",
    "\n",
    "def dC_db(w,b,x,y):\n",
    "    z1 = jnp.dot(x, w) + b\n",
    "    a1 = σ(z1)\n",
    "    fp = σ_prime(z1)\n",
    "    return jnp.average((a1-y)*fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dCw = dC_dw(w,b,x,y)\n",
    "ml4s.mdtex(rf'$\\nabla_w C = [{dCw[0]:.6f},\\, {dCw[1]:.6f}]$')\n",
    "dCb = dC_db(w,b,x,y)\n",
    "ml4s.mdtex(rf'$\\nabla_b C = {dCb:.6f}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that these gradients will allow us to step down hill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'cost before: {cost(w,b,x,y):.6f}')\n",
    "η = 0.9\n",
    "w -= η*dC_dw(w,b,x,y)\n",
    "b -= η*dC_db(w,b,x,y)\n",
    "print(f'cost after: {cost(w,b,x,y):.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It Works!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-success\">\n",
    "<h2> Programming Excercise </h2>\n",
    "    \n",
    "<ol>\n",
    "    <li>Check that our hand-coded gradients are correct using <code>jax</code>.\n",
    "    <li> Iterate over <code>num_iter = 100</code> simple gradient descent steps and plot the resulting cost vs. iteration. What do you see?\n",
    "    </li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradwC = # INSERT CODE HERE\n",
    "gradbC = # INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track the reduction in cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η = 0.9\n",
    "num_iter = 100\n",
    "\n",
    "C = []\n",
    "for n in range(num_iter):\n",
    "    # INSERT CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C)\n",
    "plt.xlabel('Iteration Step')\n",
    "plt.ylabel('Cost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
