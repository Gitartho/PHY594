{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Linear Regression - Finding the Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 06_Linear_Regression_Setup.ipynb](./06_Linear_Regression_Setup.ipynb)\n",
    "\n",
    "- Cost functions and formulating a machine learning task as an optimization problem\n",
    "- Understand linear regression \n",
    "\n",
    "## Today\n",
    "- Generalize the linear model from $1$ to $D$ dimensions\n",
    "- Minimize the cost function to extract the optimal parameter set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example from Last Time: Steady-State One-Dimensional Heat Conduction\n",
    "\n",
    "Fourier's law of heat conduction for a bar of constant cross-sectional area connected between two reservoirs in the steady-state limit gives a simple differential equation for the spatial dependence of the temperature $T$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d^2 T(x)}{d x^2} &= 0 \\\\\n",
    "\\frac{d T(x)}{dx} &= w \\\\\n",
    "T(x) &= w x + b \n",
    "\\end{align}\n",
    "\n",
    "Load experimental data from `../data/rod_temperature.dat` using the very convenient `np.loadtxt()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,T,ΔT = np.loadtxt('../data/rod_temperature.dat', unpack=True)\n",
    "plt.errorbar(x,T,ΔT, marker='o', linestyle='')\n",
    "plt.xlabel('x  (m)')\n",
    "plt.ylabel('T  (°C)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Want to predict a scalar $T$ as a function of scalar $x$ given a dataset of pairs $\\{(x^{(n)},T^{(n)})\\}_{n=1}^N$.  Here the $x^{(n)}$ are inputs and the $T^{(n)}$ are targets or observations. From physics, we have a model:\n",
    "\n",
    "\\begin{equation}\n",
    "F(x) = w x + b\n",
    "\\end{equation}\n",
    "\n",
    "i.e. $F^{(n)} = w x^{(n)} + b$.\n",
    "\n",
    "We can think of this as the simplest possible **shallow** neural network (no hidden layer) and non non-linearity, i.e. $a(x) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [[r'$x$'],[r'$F(x) = wx + b$']]\n",
    "ml4s.draw_network([1,1],weights=[np.array(['w'])],biases=[np.array(['b'])], node_labels=labels, annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to *learn* the **parameters** (weight $w$ and bias $b$) based on the **prediction** $F$ (here a linear function).  We will do this by minimizing (optimizing) a **loss** function. For a single data point (observation) this is defined to be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}^{(n)} = \\frac{1}{2} \\lvert \\lvert F^{(n)} - T^{(n)} \\rvert \\rvert^2\n",
    "\\end{equation}\n",
    "\n",
    "which quantifies the goodness of fit over our **hypothesis** space (all values of the parameters).  \n",
    "\n",
    "$F-T$ is the residual, we want to make this as small as possible, which we can do by computing the **Cost** function, the loss function averaged over all training examples (input data):\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\lvert \\lvert F^{(n)} - T^{(n)} \\rvert \\rvert^2\n",
    "}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving beyond 1 dimension\n",
    "\n",
    "Note that while we have looked at a simple 1D example, there is nothing stopping us from generalizing our model to an arbitrary number of  $D$ dimensions (i.e. think number of input neurons).  We don't even need to change our notation if we use matrix vector multiplication.  Now $F : \\mathbb{R}^D \\mapsto \\mathbb{R}$\n",
    "\n",
    "\\begin{align}\n",
    "F &= \\sum_j x_j w_j + b \\\\\n",
    "&= \\vec{x} \\cdot \\vec{w} + b \\\\\n",
    "\\end{align}\n",
    "\n",
    "where $\\vec{w}^{\\top} = (w_1,\\dots, w_{D})$ and $\\vec{x} = (x_1,\\dots, x_{D})$.   Note that our code `np.dot(x,w)+b` doesn't even need to change!  \n",
    "\n",
    "Consider the case $D = 10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 10\n",
    "N = [D,1]\n",
    "labels = [[r'$x_{' + f'{i}' + r'}$' for i in range(1,N[0]+1)],[r'$F$']]\n",
    "ml4s.draw_network(N,node_labels=labels, weights=[np.array([r'$w_{' + f'{i}' + r'}$' for i in range(1,N[0]+1)])], biases=['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can utilize the  batch processing techniques we know, where now each **row** of $\\mathbf{x}$ corresponds to a training example.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x} \\cdot \\mathbf{w} + b \\vec{1} = \n",
    "\\left(\n",
    "\\begin{array}{ccc}\n",
    "x_1^{(1)} & \\dots & x_D^{(1)} \\\\\n",
    "& \\vdots & \\\\\n",
    "x_1^{(N)} & \\dots &x_D^{(N)} \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "w_1\\\\\n",
    "\\vdots \\\\\n",
    "w_D \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "+ \n",
    "b\n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "1  \\\\\n",
    "\\vdots \\\\\n",
    "1 \n",
    "\\end{array}\n",
    "\\right)\n",
    "= \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "\\mathbf{x}^{(1)} \\cdot \\mathbf{w} + b \\\\\n",
    "\\mathbf{x}^{(2)} \\cdot \\mathbf{w} + b \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}^{(N)} \\cdot \\mathbf{w} + b\n",
    "\\end{array}\\right) = \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "F^{(1)} \\\\\n",
    "F^{(2)} \\\\\n",
    "\\vdots \\\\\n",
    "F^{(N)}\n",
    "\\end{array}\n",
    "\\right) = \\mathbf{F}\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\vec{1}$ is the $N \\times 1$ column vectors of 1's.  \n",
    "\n",
    "We can simplify our notation even further by noticing that we can incorporate the bias into the weight by tacking on a dummy input $x_0$ that always takes the value $1$ such that $w_0$ can be interpreted as the weight.  We introduce new notation ($\\mathbf{X}$ and $\\mathbf{W}$)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \n",
    "\\left(\n",
    "\\begin{array}{cccc}\n",
    "1 & x_1^{(1)} & \\dots & x_D^{(1)} \\\\\n",
    "\\vdots & \\vdots &  \\vdots & \\vdots \\\\\n",
    "1 & x_1^{(N)} & \\dots &x_D^{(N)} \\\\\n",
    "\\end{array}\n",
    "\\right) \n",
    "\\quad\n",
    "\\text{ and } \n",
    "\\quad\n",
    "\\mathbf{W} = \n",
    "\\left(\n",
    "\\begin{array}{c}\n",
    "b \\\\\n",
    "w_1\\\\\n",
    "\\vdots \\\\\n",
    "x_D \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "This allows us to write:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} \\cdot \\mathbf{W} = \\mathbf{F}\n",
    "\\end{equation}\n",
    "\n",
    "where we now interpret $w_0 = b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [D+1,1]\n",
    "labels = [[r'$x_{' + f'{i}' + r'}$' for i in range(N[0])],[r'$F$']]\n",
    "labels[0][0] = '1'\n",
    "\n",
    "ml4s.draw_network(N,node_labels=labels, weights=[np.array([r'$w_{' + f'{i}' + r'}$' for i in range(N[0])])], biases=[' '])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the squared error cost across the entire data set:\n",
    "\n",
    "\\begin{align}\n",
    "C &= \\frac{1}{2N} \\lvert\\lvert \\mathbf{F} - \\mathbf{T}\\rvert\\rvert^2 \\\\\n",
    "&= \\frac{1}{2N} \\lvert\\lvert \\mathbf{X} \\cdot \\mathbf{W} - \\mathbf{T} \\rvert\\rvert^2\n",
    "\\end{align}\n",
    "\n",
    "without modifying any of our python code.\n",
    "\n",
    "## Solving the Optimization Problem\n",
    "\n",
    "Recall, we are interested in finding the values of the weights and biases which minimize the cost function.  For the case of linear regression this can be done explicitly via calculus.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w_j} = \\frac{1}{N} \\sum_{n=1}^{N}\\left(F^{(n)} - T^{(n)} \\right) x_j^{(n)} = \\left \\langle \\left(F^{(n)} - T^{(n)} \\right) x_j^{(n)} \\right\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "The minimum occurs when this equation is set to zero, which offers a convenient closed-form solution (**you will derive this in the homework**):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W}^\\ast = \\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{T}\n",
    "\\end{equation}\n",
    "\n",
    "We can check this for our simple example of the rod temperature above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros([len(x),2]) # number of samples by 1+1 = 2 for the extra input dimension\n",
    "X[:,0] = 1 # we are assigning the first value of our input array to 1 (for all samples)\n",
    "X[:,1] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_opt = np.dot(np.dot(np.linalg.inv(np.dot(X.transpose(),X)),X.transpose()),T)\n",
    "\n",
    "# We can also write this more succintly using matrix multiplication\n",
    "W_opt = np.linalg.inv(X.T @ X) @ X.T @ T\n",
    "\n",
    "C_opt = 0.5*np.average((np.dot(X,W_opt)-T)**2)\n",
    "\n",
    "print(f'W_opt = {W_opt}')\n",
    "print(f'C_opt = {C_opt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be compared with the `np.polyfit` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.polyfit(x,T,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "They have chosen to pack their extra dimension at the end! \n",
    "</div>\n",
    "\n",
    "We can also compare the cost with a global optimization of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 100 \n",
    "weights,biases = np.meshgrid(np.linspace(400,1200,grid_size),np.linspace(-1,18,grid_size))\n",
    "C = np.zeros_like(weights)\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        F = np.dot(x,weights[i,j]) + biases[i,j]\n",
    "        C[i,j] = 0.5*np.average((F-T)**2)\n",
    "print(f'C_min = {np.min(C)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the result on the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(weights,biases,C, cmap='Spectral_r', levels=100)\n",
    "plt.plot(W_opt[1],W_opt[0], 'x', ms=10, color='k')\n",
    "\n",
    "plt.xlabel('w / (°C/m)')\n",
    "plt.ylabel('b / °C')\n",
    "plt.colorbar(label='Cost Function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Let's plot the optimal linear regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x,T,ΔT,marker='o', linestyle='', label='Exp. Data')\n",
    "\n",
    "x_fit = np.linspace(np.min(x),np.max(x),100)\n",
    "X_fit = np.zeros([x_fit.shape[0],2])\n",
    "X_fit[:,0] = 1\n",
    "X_fit[:,1] = x_fit\n",
    "\n",
    "# compute the model prediciton and plot\n",
    "F = np.dot(X_fit,W_opt)\n",
    "plt.plot(x_fit, F, color=colors[0], label='Linear Regression' )\n",
    "\n",
    "plt.xlabel('x (m)')\n",
    "plt.ylabel('T (°C)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Let's put this to use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
