{"cells":[{"cell_type":"markdown","metadata":{"id":"cSqJm1vk7eVq"},"source":["# Physics 494/594\n","## Training Neural Networks: Exercise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ixzOeOF7eVr"},"outputs":[],"source":["# %load ./include/header.py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sys\n","from tqdm import trange,tqdm\n","sys.path.append('./include')\n","import ml4s\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'svg'\n","plt.style.use('./include/notebook.mplstyle')\n","np.set_printoptions(linewidth=120)\n","ml4s.set_css_style('./include/bootstrap.css')\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"]},{"cell_type":"markdown","metadata":{"id":"pbZ0Zbo37eVs"},"source":["## Last Time\n","\n","### [Notebook Link: 16_Training_Neural_Network.ipynb](./16_Training_Neural_Network.ipynb)\n","\n","- Combine feed forward with backpropagation for supervised learning\n","- Training our deep neural network to *learn* a 2D shape\n","\n","## Today\n","\n","- Programming Exercise: the modified `XOR` function\n","\n","<div class=\"span alert alert-success\">\n","    <h3>1. Define a function that is a modified version of the <code>XOR</code> you studied on the homework:</h3>\n","    \n","\\begin{equation}\n","    \\mathtt{XOR}(x,y) =\n","    \\begin{cases}\n","    1 & (x \\ge 0 \\; \\land \\;  y < 0) \\; \\lor \\;  (x < 0 \\; \\land \\; y \\ge 0) \\\\\n","    0 & \\text{otherwise}\n","    \\end{cases}\n","\\end{equation}\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ras_3khn7eVs"},"outputs":[],"source":["def XOR(x,y):\n","    # write code for XOR\n","    return np.zeros_like(x)"]},{"cell_type":"markdown","metadata":{"id":"ci7wmclp7eVt"},"source":["<div class=\"span alert alert-success\">\n","    <h3> 2. Plot the function on a grid of at least $41 \\times 41$ points over a domain of your choice. </h3>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4iRhCxc_7eVt"},"outputs":[],"source":["grid_size = 41\n","\n","# The domain over which you want to plot [xmin,xmax,ymin,ymax]\n","extent = [-1.0, 1.0, -1.0, 1.0]\n","\n","X = np.meshgrid(np.linspace(extent[0],extent[1],grid_size),np.linspace(extent[2],extent[3],grid_size))\n","batch_size = grid_size**2\n","\n","aₒ = np.zeros([batch_size,2])\n","aₒ[:,0] = X[0].flatten()\n","aₒ[:,1] = X[1].flatten()\n","\n","# Evaluate your function here\n","# note that *X unpacks the list (https://docs.python.org/3.7/tutorial/controlflow.html#unpacking-argument-lists)\n","result = XOR(*X)\n","\n","plt.imshow(result,cmap='Spectral_r', rasterized=True, extent=extent,\n","           interpolation='nearest', origin='lower')\n","plt.xlabel(r'$x_0$')\n","plt.ylabel(r'$x_1$');"]},{"cell_type":"markdown","metadata":{"id":"5Wkw_NBf7eVt"},"source":["<div class=\"span alert alert-success\">\n","    <h3> 3. Use our backpropagation code (copied below) to train a neural network to find the weights and biases needed to reproduce the <code>XOR</code> function. </h3>\n","    \n","You can experiment with:\n","- non-linearity\n","- initial conditions\n","- batchsize\n","- learning rate\n","- topology (number of layers and neurons)\n","- learning algorithm\n","\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"NnNmHKau7eVt"},"source":["## Neural Network Training Code\n","\n","I have copied and pasted this from [16_Training_Neural_Network.ipynb](./16_Training_Neural_Network.ipynb)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"le5r2uxS7eVu"},"outputs":[],"source":["# -----------------------------------------------------------------------------------------\n","def feed_forward(aₒ,w,b):\n","    '''Propagate an input vector x = aₒ through\n","       a network with weights (w) and biases (b).\n","       Return: activations (a) and derivatives f'(z).'''\n","\n","    a,df = [aₒ],[]\n","    for wℓ,bℓ in zip(w,b):\n","        zℓ = np.dot(a[-1],wℓ) + bℓ\n","        _a,_df = ffprime(zℓ)\n","        a.append(_a)\n","        df.append(_df)\n","\n","    return a,df\n","\n","# -----------------------------------------------------------------------------------------\n","def backpropagation(y,a,w,b,df):\n","    '''Inputs: results of a forward pass\n","       Targets     y: dim(y)  = batch_size ⨯ nL\n","       Activations a: dim(a)  = L ⨯ batch_size ⨯ nℓ\n","       Weights     w: dim(w)  = L-1 ⨯ nℓ₋₁ ⨯ nℓ\n","       Biases      b: dim(b)  = L-1 ⨯ nℓ\n","       f'(z)      df: dim(df) = L-1 ⨯ batch_size ⨯ nℓ\n","\n","       Outputs: returns mini-batch averaged gradients of the cost function w.r.t. w and b\n","       dC_dw: dim(dC_dw) = dim(w)\n","       dC_db: dim(dC_db) = dim(b)\n","    '''\n","\n","    num_layers = len(w)\n","    L = num_layers-1\n","    batch_size = len(y)\n","\n","    # initialize empty lists to store the derivatives of the cost functions\n","    dC_dw = [None]*num_layers\n","    dC_db = [None]*num_layers\n","    Δ = [None]*num_layers\n","\n","    # perform the backpropagation\n","    for ℓ in range(L,-1,-1):\n","\n","        # treat the last layer differently\n","        if ℓ == L:\n","            Δ[ℓ] = (a[ℓ] - y)*df[ℓ]\n","        else:\n","            Δ[ℓ] = (Δ[ℓ+1] @ w[ℓ+1].T) * df[ℓ]\n","\n","        dC_dw[ℓ] = (a[ℓ-1].T @ Δ[ℓ]) / batch_size\n","        dC_db[ℓ] = np.average(Δ[ℓ],axis=0)\n","\n","    return dC_dw,dC_db\n","\n","# -----------------------------------------------------------------------------------------\n","def gradient_step(η,w,b,dC_dw,dC_db):\n","    '''Update the weights and biases as per gradient descent.'''\n","\n","    for ℓ in range(len(w)):\n","        w[ℓ] -= η*dC_dw[ℓ]\n","        b[ℓ] -= η*dC_db[ℓ]\n","    return w,b\n","\n","# -----------------------------------------------------------------------------------------\n","def train_network(x,y,w,b,η):\n","    '''Train a deep neural network via feed forward and back propagation.\n","       Inputs:\n","       Input         x: dim(x) = batch_size ⨯ nₒ\n","       Target        y: dim(y) = batch_size ⨯ nL\n","       Weights       w: dim(w)  = L-1 ⨯ nℓ₋₁ ⨯ nℓ\n","       Biases        b: dim(b)  = L-1 ⨯ nℓ\n","       Learning rate η\n","\n","       Outputs: the least squared cost between the network output and the targets.\n","       '''\n","\n","    a,df = feed_forward(x,w,b)\n","\n","    # we pass a cycled by 1 layer for ease of indexing (i.e. use a[-1] to get inputs)\n","    dC_dw,dC_db = backpropagation(y,a[1:]+[a[0]],w,b,df)\n","\n","    w,b = gradient_step(η,w,b,dC_dw,dC_db)\n","\n","    return 0.5*np.average((y-a[-1])**2)\n","\n","# -----------------------------------------------------------------------------------------\n","def make_batch(n,batch_size,extent,func):\n","    '''Create a mini-batch from our inputs and outputs.\n","    Inputs:\n","    n0        : number of neurons in each layer\n","    batch_size: the desired number of samples in the mini-batch\n","    extent    : [min(xₒ),max(xₒ), min(x₁),max(x₁),…,min(x_{n[0]-1}),max(x_{n[0]-1})]\n","    func:     : the desired target function.\n","\n","    Outputs: returns the desired mini-batch of inputs and targets.\n","    '''\n","\n","    x = np.zeros([batch_size,n[0]])\n","    for i in range(n[0]):\n","        x[:,i] = np.random.uniform(low=extent[2*i],high=extent[2*i+1],size=[batch_size])\n","\n","    # we expand the final axis such that y is a matrix (and not a vector)\n","    y = func(*[x[:,j] for j in range(n[0])])[:,np.newaxis]\n","\n","    return x,y"]},{"cell_type":"markdown","metadata":{"id":"B5La3hCN7eVu"},"source":["<div class=\"span alert alert-success\">\n","    <h4> Initialize your network structure </h4>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-P4p2SG7eVu"},"outputs":[],"source":["n = [2,1,1]\n","w,b = [],[]\n","for ℓ in range(len(n)-1):\n","    w.append(np.random.uniform(low=-10,high=10,size=(n[ℓ],n[ℓ+1])))\n","    b.append(np.random.uniform(low=2,high=2, size=n[ℓ+1]))"]},{"cell_type":"markdown","metadata":{"id":"XH7e1H-G7eVv"},"source":["<div class=\"span alert alert-success\">\n","    <h4>Define a non-linearity</h4>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBP-CNrr7eVv"},"outputs":[],"source":["def ffprime(z):\n","    '''calculate f(z) and f'(z).'''\n","    _f = 1.0\n","    return _f,0.0"]},{"cell_type":"markdown","metadata":{"id":"tS9Ihrg87eVv"},"source":["<div class=\"span alert alert-success\">\n","    <h4>Train your deep neural network </h4>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0FHkdAiU7eVv"},"outputs":[],"source":["η = # define learning rate\n","\n","batch_size = # batchsize\n","num_steps = # number of training epochs\n","\n","plot_ratio = int(num_steps / 50)\n","costs = np.zeros(num_steps)\n","\n","# perform the training\n","for j in range(num_steps):\n","\n","    x,y = make_batch(n,batch_size,extent,XOR)\n","    costs[j] = train_network(x,y,w,b,η)\n","\n","    # we plot every plot_ratio steps\n","    if not j % plot_ratio or j == num_steps-1:\n","        a,df = feed_forward(aₒ,w,b)\n","        aL = a[-1].reshape(X[0].shape)\n","\n","        fig,ax = ml4s.plot_training_2D(aL,XOR(*X),costs)"]},{"cell_type":"markdown","metadata":{"id":"3_Qa4C-v7eVv"},"source":["<div class=\"span alert alert-success\">\n","    <h4>Print out your final score </h4>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TcrESB5T7eVv"},"outputs":[],"source":["team_members = []\n","print(f'final cost = {np.average(costs[-10:]):.6E}')"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}