{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\left( F^{(n)} - y^{(n)} \\right)^2 = \\frac{1}{2N} \\lvert \\lvert \\vec{F} - \\vec{y}\\rvert\\rvert^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 10_Model_Complexity_Regularization.ipynb](./10_Model_Complexity_Regularization.ipynb)\n",
    "\n",
    "- Learn how linear regression can learn non-linear functions using feature maps.\n",
    "- Understanding model complexity and the bias-variance tradeoff\n",
    "- Introduction to Regularization\n",
    "\n",
    "## Today\n",
    "\n",
    "- Derive a general framework for optimizing functions of many parameters\n",
    "\n",
    "Until now, we have always assumed that we can analytically find the minimum of our cost function: \n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\left( F^{(n)} - y^{(n)} \\right)^2 = \\frac{1}{2N} \\lvert \\lvert \\vec{F} - \\vec{y}\\rvert\\rvert^2\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "which may not be possible in general.  We would like to devise a general algorithm for finding the minimum of $\\mathcal{C}$.\n",
    "\n",
    "Starting from a random point in *weight* space, $\\mathbf{w}_0$ we would like to devise an update rule $\\mathbf{w}_0 \\to \\mathbf{w}_1$ such that the the value of $\\mathcal{C}(\\mathbf{w}_i)$ always decreases.\n",
    "\n",
    "<div class=\"span alert alert-warning\">\n",
    "    <strong>Note:</strong> the subscript $j$ in $\\mathbf{w}_j$ labels a given set of weights $\\mathbf{w}_j \\equiv (w_{0,j},w_{1,j},\\dots,w_{M-1,j}) \\in \\mathbb{R}^M$.\n",
    "</div>\n",
    "\n",
    "For now, let us consider a simple function of only two variables: $\\mathbf{w}^{\\sf T} = (w_0,w_1)$\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^{\\top} \\mathsf{A}\\, \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathsf{A} \\in \\mathbb{R}^{2 \\times 2}$ is a positive semi-definite matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w,A):\n",
    "    return (1/2) * w.T @ A @ w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of this function as a quadratic bowl whose curvature is specified by the value of $A$.\n",
    "This is evident in the contour plots of $f(\\mathbf{w})$ for various $A$.  \n",
    "\n",
    "It always has a minimum at $f(\\mathbf{w}^*)=0$ given by $\\mathbf{w}^* = (0, 0)^{\\sf T}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_function(grid_1d, func, contours=50, log_contours=False, exact=[0,0]):\n",
    "    '''Make a contour plot over the region described by grid_1d for function func.'''\n",
    "    \n",
    "    # make the 2D grid\n",
    "    X,Y = np.meshgrid(grid_1d, grid_1d, indexing='xy')\n",
    "    Z = np.zeros_like(X)\n",
    "    \n",
    "    # numpy bonus exercise: can you think of a way to vectorize the following for-loop?\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X.T)):\n",
    "            Z[i, j] = func(np.array((X[i, j], Y[i, j])))  # compute function values\n",
    "    \n",
    "    fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    \n",
    "    if not log_contours:\n",
    "        ax.contour(X, Y, Z, contours, cmap='Spectral_r')\n",
    "    else:\n",
    "        ax.contour(X, Y, Z, levels=np.logspace(0, 5, 35), norm=LogNorm(), cmap='Spectral_r')\n",
    "        \n",
    "    ax.plot(*exact, '*', color='black')\n",
    "\n",
    "    ax.set_xlabel(r'$w_0$')\n",
    "    ax.set_ylabel(r'$w_1$')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    ax3d = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    \n",
    "    if log_contours:\n",
    "        Z = np.log(Z)\n",
    "        label = r'$\\ln f(\\mathbf{w}$'\n",
    "    else:\n",
    "        label = r'$f(\\mathbf{w})$'\n",
    "        \n",
    "    surf = ax3d.plot_surface(X,Y,Z, rstride=1, cstride=1, cmap='Spectral_r', \n",
    "                       linewidth=0, antialiased=True, rasterized=True)\n",
    "    \n",
    "    ax3d.plot([exact[0]], [exact[0]], [func(np.array(exact))], marker='*', ms=6, linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "         \n",
    "    ax3d.set_xlabel(r'$w_0$',labelpad=8)\n",
    "    ax3d.set_ylabel(r'$w_1$',labelpad=8)\n",
    "    ax3d.set_zlabel(label,labelpad=8);\n",
    "    \n",
    "    return fig,ax,ax3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for A in [np.ones((2, 2)), ml4s.random_psd_matrix([2,2]), ml4s.random_psd_matrix([2,2])]:\n",
    "    print(f'A={A}')\n",
    "    fig,ax,ax3d = plot_function(np.linspace(-5,5,100),lambda x: f(x,A))\n",
    "    plt.show()\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Down Hill\n",
    "\n",
    "We want to solve the following optimization problem:\n",
    "\\begin{align}\n",
    "\\mathbf{w}^* &=\\underset{\\mathbf{w}}{\\arg \\min} \\ f(\\mathbf{w}) \\\\\n",
    "& =  \\underset{\\mathbf{w}}{\\arg \\min} \\ \\frac{1}{2}\\mathbf{w}^T A \\mathbf{w}\n",
    "\\end{align}\n",
    "\n",
    "Suppose we begin at a randomly chosen point $\\mathbf{w}_0 = (w_{0,0},w_{1,0})^{\\top}$ and we propose a small change: $\\Delta \\mathbf{w} = (\\Delta w_0, \\Delta w_1)^{\\top}$ where $\\Delta w_j \\ll 1$.  From the first term in the Taylor expansion we know that the change in the function is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta f &\\equiv f(\\mathbf{w}+\\Delta\\mathbf{w}) - f(\\mathbf{w}) \\\\\n",
    "& = \\frac{\\partial f}{\\partial w_0} \\Delta w_0 + \\frac{\\partial f}{\\partial w_1} \\Delta w_1 + \\mathrm{O}\\left(|\\Delta \\mathbf{w}|^2\\right) \\\\\n",
    "&= \\nabla_w f \\cdot \\Delta \\mathbf{w} + \\mathrm{O}\\left(|\\Delta \\mathbf{w}|^2\\right)\n",
    "\\end{align}\n",
    "\n",
    "where the gradient (direction and rate of fastest increase) is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_w f = \\left(\\frac{\\partial f}{\\partial w_0},\\frac{\\partial f}{\\partial w_1} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "We can calculate this exaclty for our simple convex function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_w f(\\mathbf{w}) = \\nabla_w \\left( \\frac{1}{2}\\mathbf{w}^{\\sf T} \\mathsf{A} \\mathbf{w} \\right) = \\mathsf{A} \\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "which we can write a function for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_dw_exact(w,A):\n",
    "     return A @ w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the direction and size of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wₒ = np.random.uniform(low=-4,high=4,size=2)\n",
    "df = df_dw_exact(wₒ,A)\n",
    "\n",
    "_min = min(-5.0,wₒ[0]+df[0]-0.5,wₒ[1]+df[1]-0.5)\n",
    "_max = max(5.0,wₒ[0]+df[0]+0.5,wₒ[1]+df[1]+0.5)\n",
    "\n",
    "# plot the surface and the sampling point\n",
    "fig,ax,ax3d = plot_function(np.linspace(_min,_max,100),lambda x: f(x,A))\n",
    "ax.plot(wₒ[0],wₒ[1],'x', ms=3)\n",
    "\n",
    "# plot the gradients\n",
    "arrow_prop_dict = dict(mutation_scale=10, arrowstyle='-|>', color='k', fc='w', shrinkA=0, shrinkB=0)\n",
    "ax.annotate('',xy=wₒ+df,xytext=wₒ,xycoords='data', textcoords='data',arrowprops=dict(arrowstyle=\"-|>\", fc='w',\n",
    "                             shrinkA=0,shrinkB=0))\n",
    "a = ml4s.Arrow3D([wₒ[0], wₒ[0]+df[0]], [wₒ[1], wₒ[1]+df[1]], [f(wₒ,A),f(wₒ+df,A)], **arrow_prop_dict)\n",
    "ax3d.add_artist(a);\n",
    "\n",
    "ax.text(1,1.1,rf'$w_0 = ({wₒ[0]:.3f},{wₒ[1]:.3f}),\\; f(w_0) = {f(wₒ,A):.3f},\\;  \\nabla f(w_0) = ({df[0]:.3f},{df[1]:.3f})$', \n",
    "        fontsize=13, transform=ax.transAxes, ha='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we can guarantee that $\\Delta f$ will always tend to decrease the function if we choose:\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta \\mathbf{w} = -\\eta \\nabla_w f\n",
    "\\end{equation}\n",
    "\n",
    "where $\\eta$ is a small positive constant known as the **learning rate** such that\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta f &= \\nabla_w f \\cdot (-\\eta \\nabla_w f) \\\\\n",
    "&= -\\eta ||\\nabla_w f||^2 \\\\\n",
    "&\\le 0.\n",
    "\\end{align}\n",
    "\n",
    "Thus we can implement an iterative minimization procedure:\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{i+1} \\leftarrow \\mathbf{w}_i - \\eta \\nabla_w f(\\mathbf{w}_i).\n",
    "\\end{equation}\n",
    "\n",
    "The value of $\\eta$ controls the size of the step we take downhill. Our result will be dependent on the specific choice of $\\eta$: \n",
    "\n",
    "* if it is too large our steps will oscillate and we may miss the minimum entirely;\n",
    "* if it is too small, the minimization procedure may be very slow and never converge. \n",
    "\n",
    "Ultimately we will want to use an **adaptive** learning rate.\n",
    "\n",
    "### Jax and Autodiff\n",
    "\n",
    "To perform this Gradient Descent procedure, we require the gradient of our function $f$.  In the case considered here we were able to exactly compute the gradient.  However, in many cases (including deep neural networks) such analytical gradients are not possible and automatic differentiation packages are key!  \n",
    "\n",
    "The most important of these packages is `jax` which is **very cool** and does all kinds of things for us.  Check out the [autodiff cookbook](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp # jax has it's own accelerated version of numpy\n",
    "from jax import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dw = grad(f,argnums=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wₒ = np.array([1.0,1.0],dtype=float)\n",
    "print(f'JAX:   ∇f = {df_dw(wₒ,A)}')\n",
    "print(f'Exact: ∇f = {df_dw_exact(wₒ,A)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-danger\">\n",
    "<strong>Warning:</strong> <code>jax</code> only works for real valued (not integer) inputs)!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wₒ = np.array([1,1])\n",
    "df_dw(wₒ,A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wₒ = np.array([1,1],dtype=float)\n",
    "df_dw(wₒ,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for a range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for A in [np.ones((2, 2)), ml4s.random_psd_matrix([2,2]), ml4s.random_psd_matrix([2,2])]:\n",
    "    w = np.random.randn(2) \n",
    "    print(f'w = {w}\\nA = {A}\\nf(w,A) = {f(w,A):.3f}\\ndf/dx = {df_dw(w,A)}\\n')\n",
    "    \n",
    "    # we perform a unit test to check accuracy\n",
    "    assert np.isclose(np.sum((df_dw(w, A) - df_dw_exact(w, A)))**2, 0.0), 'Problem with auto differentiation!' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other features of `jax`\n",
    "`grad` defaults to taking the gradient with respect to the first paramter but we can specify others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dA = grad(f,argnums=1)\n",
    "print(df_dA(wₒ,A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Gradient Descent\n",
    "\n",
    "Now that we know how to take gradients using `jax` we are ready to code up our algorithm.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w}_{i+1} \\leftarrow \\mathbf{w}_i  - \\eta \\nabla_w f(\\mathbf{w}_i) \\ .\n",
    "\\end{equation}\n",
    "\n",
    "Before writing a full program, let's explore a little bit.\n",
    "\n",
    "We initialize at a random point in the domain of our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.uniform(low=-5,high=5,size=2)\n",
    "η = 0.5\n",
    "print(f'f(w) = {f(w,A):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the update step and check that we are always moving **downhill**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w -= η*df_dw(w, A)\n",
    "print(f'f(w) = {f(w,A):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "A = ml4s.random_psd_matrix([2,2], seed=0)\n",
    "fig, ax, ax3d = plot_function(np.linspace(-5, 5, 100), lambda x: f(x, A))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5\n",
    "w = np.array([2.5,-4.0])\n",
    "num_iter = 20\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # we keep a copy of the previous version for plotting\n",
    "    w_old = np.copy(w)\n",
    "    \n",
    "    # perform the GD update\n",
    "    w += -η*df_dw(w, A)\n",
    "    \n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1) \n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [f(w_old,A),f(w,A)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$f(w) = {f(w,A):.6f}$', fontsize=14);\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
