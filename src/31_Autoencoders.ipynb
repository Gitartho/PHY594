{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "\n",
    "# Autoencoders and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "Ï€ = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "- Principal Component Analysis\n",
    "- Identifying the low-dimensional latent space which maximally explains the *variance* of the data\n",
    "- Implementing PCA by hand and with `sklearn`\n",
    "\n",
    "## Today\n",
    "\n",
    "- Connection between PCA and autoencoders, a compressive deep neural network architecture\n",
    "- Application of PCA for clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Recall that for a given set of unlabelled data $\\{ \\boldsymbol{x}^{(n)} \\}_{n=0}^{N}$ our goal is to project the data onto a latent space having dimensionality $M < D$.  We did this by performing a spectral decomposition of the covariance matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma(\\mathbf{X}) = \\frac{1}{N-1} \\mathbf{X}^{\\top}\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{X}$ is the  data design matrix: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\left( \\begin{array}{cccc}\n",
    "        x_{1}^{(1)} & x_{2}^{(1)} & \\cdots & x_{D}^{(1)} \\\\\n",
    "\\vdots        &      \\vdots    & \\ddots & \\vdots \\\\\n",
    "        x_{1}^{(N)} & x_{2}^{(N)} & \\cdots & x_{D}^{(N)} \\\\\n",
    "\\end{array}\n",
    "\\right)\\, .\n",
    "\\end{equation}\n",
    "\n",
    "We determine:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{V}^\\top \\Sigma(\\mathbf{X}) \\boldsymbol{V} = \\Lambda\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Lambda_{ij} = \\lambda_i \\delta_{ij}$ is the diagonal matrix of principle components and the PCA vectors are encoded as the columns of the orthogonal matrix $\\boldsymbol{V}$.\n",
    "\n",
    "Also recall the *percentage of the explained variance* defined:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{PCA-j} = \\frac{\\lambda_j}{\\sum_{j=1}^{D} \\lambda_j}\n",
    "\\end{equation}\n",
    "\n",
    "and the projector:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{P} = \\sum_{j=1}^M\\boldsymbol{v}_j\\boldsymbol{v}_j^\\mathsf{T}\\, .\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and Linear Autoencoders\n",
    "\n",
    "There is a very nice way to interpret PCA as a type of *linear autoencoder* whereby one trains a neural network with a hidden layer (with linear activation) that acts as an **information bottleneck.**  We want to minimize the least squred error between input and output. The network calculates:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{P} \\mathbf{x}_n\n",
    "\\end{equation}\n",
    "\n",
    "for each $\\mathbf{x}_n$ and we minimize the cost:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{C} = \\left \\langle \\mathbf{x}_n^\\top \\mathbf{x}_n - \\mathbf{x}_n^\\top \\boldsymbol{P}\\mathbf{x}_n \\right \\rangle  = \\frac{1}{N} \\sum_{n=1}^{N}\\left( \\mathbf{x}_n^\\top \\mathbf{x}_n - \\mathbf{x}_n^\\top \\boldsymbol{P}\\mathbf{x}_n \\right) \\, .\n",
    "\\end{equation}\n",
    "\n",
    "To obtain the 1st princpal component for our example above we consider the linear autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml4s.draw_network([2,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt('../data/scatter_2d_pca.dat')\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(1,input_shape=(2,),activation='linear'),\n",
    "        layers.Dense(2, activation='linear')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "training_history = model.fit(x=x,y=x, epochs=100, verbose=0)\n",
    "score = model.evaluate(x, x, verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(training_history.history[\"loss\"],color=colors[0], linestyle='-', \n",
    "             label=f'cost = {score:.2f}')\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [layer.weights[0].numpy() for layer in model.layers]\n",
    "biases = [layer.weights[1].numpy() for layer in model.layers]\n",
    "ml4s.draw_network([2,1,2], weights=weights, biases=biases, zero_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x[:,0],x[:,1], s=1, alpha=0.5, label='data')\n",
    "_x = np.linspace(-4,4,100)\n",
    "\n",
    "weights = model.layers[1].weights[0].numpy()\n",
    "plt.plot(_x,weights[0][1]/weights[0][0]*_x, '-', color=colors[0], label=r'$\\mathbf{w}_1$')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you get more principal components with this strategy? \n",
    "\n",
    "Unlike the eigenvector problem above, the issue is that there is no guarentee the components will be orthognoal.  See:\n",
    "\n",
    "[E. Plaut, From Principal Subspaces to Principal Components with Linear Autoencoders, arXiv:1804.10253 (2018)](https://arxiv.org/abs/1804.10253)\n",
    "\n",
    "for a discussion of how you can re-orthogonalize via a singular value decomposition of the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(2,input_shape=(2,),activation='linear'),\n",
    "        layers.Dense(2, activation='linear')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "training_history = model.fit(x=x,y=x, epochs=100, verbose=0)\n",
    "score = model.evaluate(x, x, verbose=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(training_history.history[\"loss\"],color=colors[0], linestyle='-', \n",
    "             label=f'cost = {score:.2f}')\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "ax[0].scatter(x[:,0],x[:,1], s=1, alpha=0.5,label='data')\n",
    "\n",
    "_x = np.linspace(-1,1,100)\n",
    "\n",
    "weights = model.layers[1].weights[0].numpy()\n",
    "ax[0].plot(_x,weights[0][1]/weights[0][0]*_x, '-', color=colors[0], label=r'$\\mathbf{w}_1$')\n",
    "ax[0].plot(_x,weights[1][1]/weights[1][0]*_x, '-', color=colors[-2], label=r'$\\mathbf{w}_2$')\n",
    "\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].scatter(x[:,0],x[:,1], s=1, alpha=0.5)\n",
    "\n",
    "u, s, vh = scipy.linalg.svd(weights)\n",
    "u = np.flip(u,axis=1)\n",
    "\n",
    "ax[1].plot(_x,u[1,0]/u[0,0]*_x, '-', color=colors[0], label=r'$\\mathbf{u}_1$')\n",
    "ax[1].plot(_x,u[1,1]/u[0,1]*_x, '-', color=colors[-2], label=r'$\\mathbf{u}_2$')\n",
    "\n",
    "ax[1].axis('equal')\n",
    "ax[1].legend()\n",
    "ax[1].set_xticks([])\n",
    "ax[1].set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis for MNIST\n",
    "\n",
    "Let's try to understand this for a *real* dataset, MNIST.\n",
    "\n",
    "We begin by loading the (now familiar) MNIST dataset, 60000 $28\\times 28$ images of hand-written digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# determine the properties\n",
    "rows,cols = x_train[0].shape\n",
    "\n",
    "# reshape and rescale\n",
    "x_train = x_train.reshape(x_train.shape[0], rows*cols).astype('float32')/255\n",
    "x_test = x_test.reshape(x_test.shape[0], rows*cols).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# perform the PCA, keeping M components\n",
    "M = 100\n",
    "model = PCA(n_components=M)\n",
    "XPCA = model.fit_transform(x_train)\n",
    "\n",
    "# store the results\n",
    "Î» = model.explained_variance_\n",
    "PCAj = model.explained_variance_ratio_\n",
    "V = model.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the Explained Variance Ratio\n",
    "\n",
    "Over 90% of the variance is explained by the first 100 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(PCAj)\n",
    "ax.set_ylabel('PCA-j')\n",
    "ax.set_xlabel('Component');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the PCA eigenvectors\n",
    "\n",
    "We can project and plot the PCA eigenvectors to get a sense of the **important** features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digit_array(x,y, show_prediction=False):\n",
    "    '''Expects a list of digits (x) and associated labels (y)'''\n",
    "    \n",
    "    # determine the number of rows and columns of our image array\n",
    "    num_digits = x.shape[0]\n",
    "    num_cols = int(np.sqrt(num_digits))\n",
    "    num_rows = num_digits//num_cols + 1\n",
    "\n",
    "    fig,ax = plt.subplots(nrows=num_rows,ncols=num_cols,sharex=True,sharey=True,\n",
    "                          figsize=(num_cols,num_rows))\n",
    "    \n",
    "    # plot all the numbers\n",
    "    for i,cax in enumerate(ax.flatten()):\n",
    "        if i < num_digits:\n",
    "            cax.matshow(x[i].reshape(28,28), cmap='binary')\n",
    "            cax.axis('off')\n",
    "            if show_prediction:\n",
    "                cax.text(0.99,0.99,f'{y[i]}',horizontalalignment='right',verticalalignment='top', \n",
    "                         transform=cax.transAxes, fontsize=8, color='r')\n",
    "        else:\n",
    "            cax.axis('off')\n",
    "            \n",
    "plot_digit_array(V[:20],range(20), show_prediction=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensional Reduction & Clustering\n",
    "\n",
    "We can also use the PCA to project the MNIST data set into a lower dimensional ($M=2$) space where can visually inspect for patterns and clusters.\n",
    "\n",
    "We first form the matrix of PCA vectors $V$ from which we can construct the projection operator $\\boldsymbol{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pX = x_train @ V[:2,:].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "scatter = ax.scatter(pX[:,0],pX[:,1],s=2,c=y_train,cmap='Spectral_r')\n",
    "\n",
    "# produce a legend with the unique colors corresponding to digits\n",
    "legend = ax.legend(*scatter.legend_elements(),loc=(1,0), title=\"MNIST Digits\")\n",
    "ax.add_artist(legend)\n",
    "\n",
    "ax.set_xlabel(f'PCA-1 = {PCAj[0]:.2f}')\n",
    "ax.set_ylabel(f'PCA-2 = {PCAj[1]:.2f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly there are patterns, but there is also overlap. This is related to the curse of high-dimensionality as it is very difficult to preserve distances between points when projecting into low-dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
