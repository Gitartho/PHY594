{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "import jax.numpy as jnp \n",
    "from jax import grad\n",
    "from IPython import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "π = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 10_Model_Complexity_Regularization.ipynb](./10_Model_Complexity_Regularization.ipynb)\n",
    "\n",
    "- Improved gradient descent methods based on momentum and adaptation\n",
    "\n",
    "## Today\n",
    "\n",
    "- Introduction of approximate  gradients for stochastic downhill steps.\n",
    "\n",
    "Let us return to our original task, the minimization of a cost function: \n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\n",
    "\\mathcal{C} = \\frac{1}{2N} \\sum_{n=1}^N  \\left( F^{(n)} - y^{(n)} \\right)^2 = \\frac{1}{2N} \\lvert \\lvert \\vec{F} - \\vec{y}\\rvert\\rvert^2\n",
    "}\n",
    "\\end{equation}\n",
    "\n",
    "which represents the goodness of fit of some data set $\\{(x^{(n)},y^{(n)})\\}_{n=1}^N$ to some proposed model $F$. Here, for simplicity we will assume $x^{(n)},y^{(n)} \\in \\mathbb{R}$ such that  $F : \\mathbb{R} \\mapsto \\mathbb{R}$.\n",
    "\n",
    "Consider the following data set located at `../data/stochastic_GD.dat`\n",
    "\n",
    "<!--\n",
    "x = np.linspace(0,6.0/(π),100)\n",
    "header = f\"{'x':>13s}\\t{'y':>15s}\"\n",
    "k1 = np.random.normal(loc=2*π,scale=0.1,size=x.size)\n",
    "k2 = np.random.normal(loc=4*π,scale=0.2,size=x.size)\n",
    "y = np.cos(k1*x)*np.sin(k2*x) + np.random.normal(loc=0,scale=0.05,size=x.size)\n",
    "data_out = np.column_stack([x,y])\n",
    "np.savetxt('../data/stochastic_GD.dat', data_out,fmt='% 15.8e', header=header, delimiter='\\t')\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/stochastic_GD.dat',unpack=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The proposed model to fit this data is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "F(x,\\vec{w}) = \\cos(w_0 x)\\sin(w_1 x)\n",
    "\\end{equation}\n",
    "\n",
    "Our goal is to extract the weights $\\vec{w}$ from the data.\n",
    "\n",
    "<div class=\"span alert alert-warning\">\n",
    "    <strong>Note:</strong> if we want to use <tt>jax</tt> we need to use <tt>jnp</tt>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F(x,w):\n",
    "    return jnp.cos(w[0]*x)*jnp.sin(w[1]*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cost Function \n",
    "\n",
    "We define the cost function as the least squares difference (maximum likelihood) between the model and data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def C(w,x,y):\n",
    "    return 0.5*jnp.average((F(x,w)-y)**2)\n",
    "    \n",
    "dC_dw = grad(C,argnums=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualze\n",
    "\n",
    "As before, we can visualize the cost function as a 2D convex function in weight space.  Let's use our previously defined functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_w = np.linspace(π,5*π,50)\n",
    "fig,ax,ax3d = ml4s.plot_2D_function(grid_w,grid_w,lambda w: C(w,x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the direction and size of the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_w = np.min(grid_w)\n",
    "max_w = np.max(grid_w)\n",
    "wₒ = np.random.uniform(low=min_w,high=max_w,size=2)\n",
    "\n",
    "dC = dC_dw(wₒ,x,y)\n",
    "\n",
    "_min = min(min_w,wₒ[0]+dC[0]-0.5,wₒ[1]+dC[1]-0.5)\n",
    "_max = max(max_w,wₒ[0]+dC[0]+0.5,wₒ[1]+dC[1]+0.5)\n",
    "\n",
    "# plot the surface and the sampling point\n",
    "fig,ax,ax3d = ml4s.plot_2D_function(grid_w,grid_w,lambda w: C(w,x,y))\n",
    "ax.plot(wₒ[0],wₒ[1],'x', ms=5)\n",
    "\n",
    "# plot the gradients\n",
    "arrow_prop_dict = dict(mutation_scale=10, arrowstyle='-|>', color='k', fc='w', shrinkA=0, shrinkB=0)\n",
    "ax.annotate('',xy=wₒ+dC,xytext=wₒ,xycoords='data', textcoords='data',arrowprops=dict(arrowstyle=\"-|>\", fc='w',\n",
    "                             shrinkA=0,shrinkB=0))\n",
    "a = ml4s.Arrow3D([wₒ[0], wₒ[0]+dC[0]], [wₒ[1], wₒ[1]+dC[1]], [C(wₒ,x,y),C(wₒ+dC,x,y)], **arrow_prop_dict)\n",
    "ax3d.add_artist(a);\n",
    "\n",
    "ax.text(1,1.1,rf'$w_0 = ({wₒ[0]:.3f},{wₒ[1]:.3f}),\\; C(w_0) = {C(wₒ,x,y):.3f},\\;  \\nabla C(w_0) = ({dC[0]:.3f},{dC[1]:.3f})$', \n",
    "        fontsize=13, transform=ax.transAxes, ha='center');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Implement our convential gradient descent procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax,ax3d = ml4s.plot_2D_function(grid_w,grid_w,lambda w: C(w,x,y))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5\n",
    "num_iter = 100\n",
    "\n",
    "# let's start near the minimum\n",
    "w = np.array([6.75,14.5])\n",
    "\n",
    "# store the cost function during the trajectory\n",
    "C_traj = {}\n",
    "C_traj['GD'] = np.zeros([num_iter])\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "\n",
    "for i in range(num_iter):\n",
    "\n",
    "    # we keep a copy of the previous version for plotting\n",
    "    w_old = np.copy(w)\n",
    "    C_traj['GD'][i] = C(w,x,y)\n",
    "    \n",
    "    # perform the GD update\n",
    "    w += -η*dC_dw(w,x,y)\n",
    "    \n",
    "    # plot\n",
    "    ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1) \n",
    "    ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [C(w_old,x,y),C(w,x,y)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "    ax.set_title(f'$i={i}, w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$C(w) = {C(w,x,y):.6f}$', fontsize=14);\n",
    "    display.display(fig)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the cost function during minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C_traj['GD'], label='Gradient Descent')\n",
    "plt.xlabel('Iteration Step')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Instead of computing the gradient using the entire data set $\\mathcal{D} = \\{(x^{(n)},y^{(n)})\\}_{n=1}^N$ we select a minibatch of size $N_B$.  Let's take $N_B = 5$ here.  We start by computing the random indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = x.size\n",
    "NB = 5\n",
    "idx = np.random.choice(N, NB, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the exact and approximate gradient for a minibatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wₒ = np.random.uniform(low=min_w,high=max_w,size=2)\n",
    "\n",
    "print('Exact:')\n",
    "dC = dC_dw(wₒ,x,y)\n",
    "out = rf'$w_0 = ({wₒ[0]:.3f},{wₒ[1]:.3f}),\\; C(w_0) = {C(wₒ,x,y):.3f},\\;  \\nabla C(w_0) = ({dC[0]:.3f},{dC[1]:.3f})$'\n",
    "ml4s.mdtex(out)\n",
    "\n",
    "print('MiniBatch:')\n",
    "dC = dC_dw(wₒ,x[idx],y[idx])\n",
    "out = rf'$w_0 = ({wₒ[0]:.3f},{wₒ[1]:.3f}),\\; C(w_0) = {C(wₒ,x,y):.3f},\\;  \\nabla C(w_0) = ({dC[0]:.3f},{dC[1]:.3f})$'\n",
    "ml4s.mdtex(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax,ax3d = ml4s.plot_2D_function(grid_w,grid_w,lambda w: C(w,x,y))\n",
    "\n",
    "# hyperparameters\n",
    "η = 0.5\n",
    "w = np.array([6.75,14.5])\n",
    "\n",
    "num_epoch = 5\n",
    "num_batch = int(N/NB)\n",
    "\n",
    "w_traj = np.zeros([num_epoch*num_batch,2])\n",
    "w_traj[0,:] = w\n",
    "\n",
    "C_traj['SGD'] = np.zeros([num_epoch*num_batch])\n",
    "\n",
    "ax.plot(*w, marker='.', color='k', ms=15)  \n",
    "\n",
    "# each epoch includes all minibatches\n",
    "i = 0\n",
    "for epoch in range(num_epoch):\n",
    "    for batch in range(num_batch):\n",
    "\n",
    "        # get the batch\n",
    "        idx = np.random.choice(N, NB, replace=False)\n",
    "        \n",
    "        # we keep a copy of the previous version for plotting\n",
    "        w_old = np.copy(w)\n",
    "        C_traj['SGD'][i] = C(w,x,y)\n",
    "\n",
    "        # perform the stocahstic GD update\n",
    "        w += -η*dC_dw(w,x[idx],y[idx])\n",
    "\n",
    "        # plot\n",
    "        ax.plot([w_old[0], w[0]], [w_old[1], w[1]], marker='.', linestyle='-', color='k',lw=1) \n",
    "        ax3d.plot([w_old[0], w[0]], [w_old[1], w[1]], [C(w_old,x,y),C(w,x,y)], marker='.', linestyle='-', color='k',lw=1, zorder=100)\n",
    "\n",
    "        ax.set_title(fr'$w=[{w[0]:.2f},{w[1]:.2f}]$' + '\\n' + f'$C(w) = {C(w,x,y):.6f}$', fontsize=14);\n",
    "        display.display(fig)\n",
    "        display.clear_output(wait=True)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C_traj['GD'], label='Gradient Descent')\n",
    "plt.plot(C_traj['SGD'], label='Stochastic Gradient Descent')\n",
    "\n",
    "plt.xlabel('Iteration Step')\n",
    "plt.ylabel('Cost')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It Works!\n",
    "\n",
    "Let's return to our original model and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y, 'o', label='data', alpha=0.5)\n",
    "plt.plot(x,F(x,w),'-', label='fit', color=colors[0])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
