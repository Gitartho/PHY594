{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Exploring Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 09_Feature_Maps.ipynb](./09_Feature_Maps.ipynb)\n",
    "\n",
    "-  Learn how linear regression can be used to fit data to non-linear functions using feature maps.\n",
    "\n",
    "## Today\n",
    "\n",
    "- Understanding model complexity and the bias-variance tradeoff\n",
    "- Introduction to Regularization\n",
    "\n",
    "### Polynomial Fitting\n",
    "\n",
    "We used feature maps to implement a model consisting of a 3rd order polynomial:\n",
    "\n",
    "\\begin{align}\n",
    "F_3(\\vec{w},x) &= \\vec{\\varphi}(x) \\cdot \\vec{w}  \\\\\n",
    "&= w_0 + w_1 x + w_2 x^2 + w_3 x^3\n",
    "\\end{align}\n",
    "\n",
    "where \\begin{equation}\n",
    "\\vec{w} = \\left( \\begin{array}{c}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_{3}\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\varphi}(x) = \\left(\n",
    "x^0, \n",
    "x^1,\n",
    "x^2,\n",
    "x^3\n",
    "\\right)\\, .\n",
    "\\end{equation}\n",
    "\n",
    "This was implemented as a new non-linear processing layer in our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [1,4,1]\n",
    "labels = [[r'$x$'],['1','$x$','$x^2$','$x^3$'],[r'$F(x,\\vec{w})$']]\n",
    "ml4s.draw_network(N,node_labels=labels, weights=[['' for i in range(N[1])],[f'$w_{i}$' for i in range(N[1])]], biases=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded data from disk that we wanted to fit to the model:\n",
    "\n",
    "\n",
    "<!--\n",
    "x = np.linspace(0,1,10)\n",
    "header = f\"{'x':>13s}\\t{'y':>15s}\"\n",
    "data_out = np.column_stack([x,np.sin(2*np.pi*x)+np.random.normal(loc=0,scale=0.15,size=x.size)])\n",
    "np.savetxt('../data/poly_regression.dat', data_out,fmt='% 15.8e', header=header, delimiter='\\t')\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/poly_regression.dat',unpack=True)\n",
    "plt.plot(x,y, 'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Model Complexity\n",
    "\n",
    "Last time we guessed the order of the polynomial for regression.  We can systematically explore the fit as a function of the order of the polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data\n",
    "plt.plot(x,y, 'o', label='data', color=colors[0])\n",
    "\n",
    "# the polynomial fits\n",
    "W_opt = []\n",
    "C_opt = []\n",
    "x_fit = np.linspace(np.min(x),np.max(x),100)\n",
    "\n",
    "poly_order = [0,4,9]\n",
    "\n",
    "for n,cpoly_order in enumerate(poly_order):\n",
    "    Φ = np.zeros([len(x),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ[:,j] = x**j\n",
    "        \n",
    "    W_opt.append(np.linalg.inv(Φ.T @ Φ) @ Φ.T @ y)\n",
    "    C_opt.append(0.5*np.average((Φ @ W_opt[n]-y)**2))\n",
    "    \n",
    "    Φ_fit = np.zeros([len(x_fit),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ_fit[:,j] = x_fit**j\n",
    "\n",
    "    plt.plot(x_fit,np.dot(Φ_fit,W_opt[n]),'-', color=colors[2*n+1], label=f'order = {poly_order[n]}; cost = {C_opt[n]:.1e}' )\n",
    "        \n",
    "plt.legend(loc=(1,0.0))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we can observe two extremes:\n",
    "\n",
    "**Underfitting:** Model is too simple – does not fit the data.\n",
    "\n",
    "**Overfitting:** Model is too complex – fits perfectly but does not generalize.\n",
    "\n",
    "We can explore this behavior systematically by employing knowledge from **Statistical Learning Theory** (see [Online Course](https://work.caltech.edu/telecourse) and associated textbook by Yaser Abu-Mostafa).\n",
    "\n",
    "Let us consider our previous example (noisy $\\sin(2\\pi x)$) but with much more data ($N = 1000$):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{D} = \\{(x^{(n)},y^{(n)})\\}_{n=1}^{1000}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "The **first step** is to randomly divide our data set $\\mathcal{D}$ into two mutually exclusive groups $\\mathcal{D}_{\\rm train}$ and $\\mathcal{D}_{\\rm test}$.  Can do this with built-in `numpy` functions.  No hard-and-fast rule for the relative sizes (we are encountering one of our first *hyperparameters*), 80% is a good place to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/poly_regression_long.dat',unpack=True)\n",
    "\n",
    "N_train = int(0.8*x.size)\n",
    "indices = np.random.permutation(x.shape[0])\n",
    "training_idx, test_idx = indices[:N_train], indices[N_train:]\n",
    "x_train,x_test = x[training_idx],x[test_idx]\n",
    "y_train,y_test = y[training_idx],y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it is often simpler to use pre-canned versions from machine-learning libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = np.loadtxt('../data/poly_regression_long.dat',unpack=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_train,y_train, 'o', ms=2, label='Training Data')\n",
    "plt.plot(x_test,y_test, 'd', ms=2, label='Testing Data')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then minimize the cost function using **only** data in the training set.  This gives us two different measurements of error:\n",
    "\n",
    "#### In-Sample (training) Error\n",
    "\\begin{equation}\n",
    "E_{\\rm in} = \\mathcal{C}\\left(\\vec{y}_{\\rm train},F(\\vec{x}_{\\rm train}, \\vec{w}^\\ast)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "#### Out-of-sample (testing) Error\n",
    "\\begin{equation}\n",
    "E_{\\rm out} = \\mathcal{C}\\left(\\vec{y}_{\\rm test},F(\\vec{x}_{\\rm test}, \\vec{w}^\\ast)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "It is **almost always the case** that out-of-sample error is greater than in-sample error.\n",
    "\n",
    "\\begin{equation}\n",
    "E_{\\rm out} \\ge E_{\\rm in}.\n",
    "\\end{equation}\n",
    "\n",
    "Splitting the data into mutually exclusive training and test sets provides an unbiased estimate for the predictive performance of the model — this is known as cross-validation in the ML and statistics literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_opt = []\n",
    "E_in,E_out = [],[]\n",
    "\n",
    "poly_order = np.arange(16)\n",
    "\n",
    "for n,cpoly_order in enumerate(poly_order):\n",
    "    \n",
    "    # training\n",
    "    Φ = np.zeros([len(x_train),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ[:,j] = x_train**j\n",
    "        \n",
    "    W_opt.append(np.linalg.inv(Φ.T @ Φ) @ Φ.T @ y_train)\n",
    "    \n",
    "    # in-sample (training) error\n",
    "    E_in.append(0.5*np.average((Φ @ W_opt[n] - y_train)**2))\n",
    "    \n",
    "    # out-of-sample (testing) error\n",
    "    Φ = np.zeros([len(x_test),cpoly_order+1])\n",
    "    for j in range(Φ.shape[1]):\n",
    "        Φ[:,j] = x_test**j\n",
    "    E_out.append(0.5*np.average((Φ @ W_opt[n] - y_test)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Training and Testing Errors as a function of Model Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(poly_order,E_in, marker='o', ms=4, label=r'$E_{\\rm in}$')\n",
    "plt.plot(poly_order,E_out, marker='o',ms=4, label=r'$E_{\\rm out}$')\n",
    "\n",
    "plt.xlabel('Polynomial Order (Model Complexity)')\n",
    "plt.ylabel('Error')\n",
    "plt.xticks(np.arange(0,np.max(poly_order)+1))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Least Squares\n",
    "\n",
    "When we enter a regime of **overfitting** one consistent observation is that the fitting parameters tend to grow very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ml4s.get_linear_colors('Spectral_r',len(poly_order))\n",
    "for i,cW in enumerate(W_opt):\n",
    "    plt.plot(np.abs(cW), '-o', ms=5, lw=1, color=colors[i], label=f'Fit Order: {i}')\n",
    "plt.yscale('log')\n",
    "plt.xticks(np.arange(0,np.max(poly_order)+1))\n",
    "plt.ylabel(r'$w_j$')\n",
    "plt.legend(ncol=2, loc=(1.1,0))\n",
    "plt.xlabel(r'Power Index $j$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to address this is just to punish large values of the weights in a modified cost function:\n",
    "\n",
    "\\begin{equation}\n",
    "C_{\\rm ridge}(\\vec{w}) = \\frac{1}{2}|| \\vec{F}(\\vec{w})-\\vec{y}||^2 + \\frac{\\lambda}{2} ||\\vec{w}||^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is a *regularization constant*.  This is another hyperparameters that we need to choose by investigation.  Different powers of the regression term can act differently ($||\\vec{w}||$ is called *lasso* regression in the statistics literature).  For the choice above, our equation for the optimal parameters above is modified to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{w}^\\ast = \\left(\\lambda \\mathbb{1} + \\mathbf{\\Phi}^{\\sf T} \\mathbf{\\Phi}\\right)^{-1} \\mathbf{\\Phi}^{\\sf T} \\vec{y}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbb{1}$ is the $M\\times M$ identity matrix.  Note the fact that the regularizer now prevents divergences if $\\mathbf{\\Phi}^{\\sf T} \\mathbf{\\Phi}$ becomes singular.  We can easily modify our code above to use this regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-success\">\n",
    "<h2>Programming Exercise: Ridge Regression </h2>\n",
    "\n",
    "Adapt our analysis above to use use ridge regression.  Play around with different values of $\\lambda$ to observe their effects.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
