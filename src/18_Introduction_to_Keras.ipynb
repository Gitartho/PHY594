{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Introduction to Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 16_Training_Neural_Network.ipynb](./16_Training_Neural_Network.ipynb)\n",
    "\n",
    "- Combine feed forward with backpropagation for supervised learning\n",
    "- Training our deep neural network to *learn* a 2D shape\n",
    "\n",
    "## Today\n",
    "\n",
    "- Learn how to use the `keras` and `tensorflow` libraries to build sequential deep neural networks.\n",
    "- Learn a simple 2D logical function\n",
    "\n",
    "### Import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've already seen `AND` let's learn a simple cross \"+\" function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross(x,y):\n",
    "    return (x>-0.25)*(x<0.25) + (y>-0.25)*(y<0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the function to be learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 41\n",
    "\n",
    "# The domain over which you want to plot [xmin,xmax,ymin,ymax]\n",
    "extent = [-1.0, 1.0, -1.0, 1.0]\n",
    "\n",
    "X = np.meshgrid(np.linspace(extent[0],extent[1],grid_size),np.linspace(extent[2],extent[3],grid_size))\n",
    "batch_size = grid_size**2\n",
    "\n",
    "aₒ = np.zeros([batch_size,2])\n",
    "aₒ[:,0] = X[0].flatten()\n",
    "aₒ[:,1] = X[1].flatten()\n",
    "\n",
    "# Evaluate your function here\n",
    "# note that *X unpacks the list (https://docs.python.org/3.7/tutorial/controlflow.html#unpacking-argument-lists)\n",
    "result = cross(*X)\n",
    "\n",
    "plt.imshow(result,cmap='Spectral_r', rasterized=True, extent=extent,\n",
    "           interpolation='nearest', origin='lower')\n",
    "plt.xlabel(r'$x_0$')\n",
    "plt.ylabel(r'$x_1$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(n,batch_size,extent,func):\n",
    "    '''Create a mini-batch from our inputs and outputs.\n",
    "    Inputs:\n",
    "    n         : number of neurons in each layer\n",
    "    batch_size: the desired number of samples in the mini-batch\n",
    "    extent    : [min(xₒ),max(xₒ), min(x₁),max(x₁),…,min(x_{n[0]-1}),max(x_{n[0]-1})]\n",
    "    func:     : the desired target function.\n",
    "    \n",
    "    Outputs: returns the desired mini-batch of inputs and targets.\n",
    "    '''\n",
    "    \n",
    "    # n[0] is the input dimension nₒ\n",
    "    x = np.zeros([batch_size,n[0]])\n",
    "    for i in range(n[0]):\n",
    "        x[:,i] = np.random.uniform(low=extent[2*i],high=extent[2*i+1],size=[batch_size])\n",
    "\n",
    "    # we expand the final axis such that y is a matrix (and not a vector)\n",
    "    y = func(*[x[:,j] for j in range(n[0])])[:,np.newaxis]\n",
    "    \n",
    "    return x,y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the deep neural network in `keras`\n",
    "\n",
    "This is a `sequential` network as it is made up of 1 stack of layers, each with a single input and output layer.  I encourage you to use the extensive documentation available at [tensorflow.org](https://www.tensorflow.org/guide).  `keras` is the high-level API that makes it very easy to build and train neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network topology (architecture)\n",
    "n = [2,10,4,1]\n",
    "\n",
    "# initilize the model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# the input layer is treated differently\n",
    "model.add(keras.Input(shape=(n[0],)))\n",
    "\n",
    "#  Create an initilizer for the weights\n",
    "initializer = tf.keras.initializers.RandomUniform(minval=-2.0,maxval=2.0, seed=42)\n",
    "\n",
    "# construct and initialize the network\n",
    "for i,nℓ in enumerate(n[1:]):\n",
    "    model.add(layers.Dense(nℓ, activation='sigmoid',kernel_initializer=initializer, \n",
    "                           bias_initializer=initializer))\n",
    "\n",
    "# setup the properties of the 'model'\n",
    "η = 0.9\n",
    "SGD = keras.optimizers.SGD(learning_rate=η)\n",
    "model.compile(loss='mean_squared_error', optimizer=SGD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a glimpse at your network via `model.summary()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in graph form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [layer.weights[0].numpy() for layer in model.layers]\n",
    "biases = [layer.weights[1].numpy() for layer in model.layers]\n",
    "ml4s.draw_network(n, weights=weights, biases=biases, zero_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feed forward is just evaluating the model on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aL = model(aₒ).numpy().reshape(grid_size,grid_size)\n",
    "plt.imshow(aL, extent=extent, cmap='Spectral_r', rasterized=True, \n",
    "           interpolation='nearest', origin='lower', aspect='equal')\n",
    "plt.xlabel(r'$x_0$')\n",
    "plt.ylabel(r'$x_1$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "num_steps = 5000\n",
    "plot_ratio = int(num_steps / 50)\n",
    "\n",
    "costs = np.zeros(num_steps)\n",
    "for j in range(num_steps):\n",
    "    \n",
    "    x,y = make_batch(n,batch_size,extent,cross)\n",
    "    costs[j] = model.train_on_batch(x,y)\n",
    "    \n",
    "    # we plot every plot_ratio steps\n",
    "    if not j % plot_ratio or j == num_steps-1:\n",
    "        \n",
    "        aL = model(aₒ).numpy().reshape(X[0].shape)      \n",
    "        fig,ax = ml4s.plot_training_2D(aL,cross(*X),costs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also have keras train on auto-generated batches\n",
    "\n",
    "#### 1. Load and process all the data; we wille have `keras` make the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = make_batch([2,1],10000,extent,cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- The following code is used to interogate our model while training, it is not needed\n",
    "!rm -rf ./logs/\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setup our model and define the architecture\n",
    "\n",
    "We can auto-generate the input layer using `input_shape` in our first hidden layer.  A list of possible activation functions can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/activations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(4,input_shape=(2,),activation='relu'),\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        layers.Dense(4, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Choose the cost function (loss) and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = {}\n",
    "training_history['adam'] = model.fit(x=x,y=y, epochs=30,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Study the success of the model, make sure to include performance on the unseen validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=3,nrows=1,figsize=(10,4))\n",
    "        \n",
    "ax[1].axis('off')\n",
    "\n",
    "aL = model(aₒ).numpy().reshape(grid_size,grid_size)\n",
    "\n",
    "img = ax[1].imshow(aL, extent=extent, cmap='Spectral_r', rasterized=True, \n",
    "       interpolation='nearest', origin='lower', aspect='equal',vmin=0, vmax=1)\n",
    "\n",
    "ax[2].axis('off')\n",
    "ax[2].imshow(cross(*X), extent=extent, cmap='Spectral_r', rasterized=True, \n",
    "   interpolation='nearest', origin='lower', aspect='equal',vmin=0, vmax=1)\n",
    "\n",
    "ax[0].plot(training_history['adam'].history['loss'], label=f'train = {np.average(training_history[\"adam\"].history[\"loss\"][-10:]):.2g}')\n",
    "ax[0].plot(training_history['adam'].history[\"val_loss\"], label=f'test = {np.average(training_history[\"adam\"].history[\"val_loss\"][-10:]):.2g}')\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "ax[0].set_title(\"Cost\")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[1].set_title(\"Model Prediction\")\n",
    "ax[2].set_title(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Optimize performance for the data set by changing hyperparameters\n",
    "\n",
    "Let's investigate the difference between SGD and adam for the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model():\n",
    "    return keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(4,input_shape=(2,),activation='relu'),\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        layers.Dense(4, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "η = [0.0001,0.001,0.01,0.05,0.1,1]\n",
    "for cη in tqdm(η):\n",
    "    model = generate_model()\n",
    "    SGD = keras.optimizers.SGD(learning_rate=cη)\n",
    "    model.compile(loss='mean_squared_error', optimizer=SGD)\n",
    "    training_history[f'SGD η = {cη}'] = model.fit(x=x,y=y, epochs=30,validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(30)\n",
    "\n",
    "plt.plot(epochs,training_history['adam'].history[\"loss\"], label=f'adam', color='gray', linestyle='-')\n",
    "plt.plot(epochs+0.5,training_history['adam'].history[\"val_loss\"], color='gray', linestyle='--')\n",
    "\n",
    "for i,cη in enumerate(η):\n",
    "    plt.plot(epochs,training_history[f'SGD η = {cη}'].history[\"loss\"], label=f'SGD η = {cη}', color=colors[i], linestyle='-')\n",
    "    plt.plot(epochs+0.5,training_history[f'SGD η = {cη}'].history[\"val_loss\"], color=colors[i], linestyle='--')\n",
    "\n",
    "plt.legend(loc=(1,0.3))\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can compare different cost functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(4,input_shape=(2,),activation='relu'),\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        layers.Dense(4, activation='relu'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "training_history[f'absolute'] = model.fit(x=x,y=y, epochs=30,validation_split=0.2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,training_history['adam'].history[\"loss\"], label=f'MSE', color=colors[0], linestyle='-')\n",
    "plt.plot(epochs+0.5,training_history['adam'].history[\"val_loss\"], color=colors[0], linestyle='--')\n",
    "\n",
    "plt.plot(epochs,training_history['absolute'].history[\"loss\"], label=f'MAE', color=colors[-1], linestyle='-')\n",
    "plt.plot(epochs+0.5,training_history['absolute'].history[\"val_loss\"], color=colors[-1], linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or different layer types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(4,input_shape=(2,),activation='sigmoid'),\n",
    "        layers.Dense(20, activation='sigmoid'),\n",
    "        layers.Dense(4, activation='sigmoid'),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "training_history['sigmoid'] = model.fit(x=x,y=y, epochs=30,validation_split=0.1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs,training_history['adam'].history[\"loss\"], label=f'relu', color=colors[0], linestyle='-')\n",
    "plt.plot(epochs,training_history['adam'].history[\"val_loss\"], color=colors[0], linestyle='--')\n",
    "\n",
    "plt.plot(epochs,training_history['sigmoid'].history[\"loss\"], label=f'sigmoid', color=colors[-1], linestyle='-')\n",
    "plt.plot(epochs,training_history['sigmoid'].history[\"val_loss\"], color=colors[-1], linestyle='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.xlabel(\"Epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ml4s)",
   "language": "python",
   "name": "ml4s"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
