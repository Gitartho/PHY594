{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "## Convolutional Networks Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "π = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time\n",
    "\n",
    "### [Notebook Link: 24_Image_Filters.ipynb](./24_Image_Filters.ipynb)\n",
    "\n",
    "- Image filters: local feature detection and sampling\n",
    "\n",
    "## Today\n",
    "\n",
    "- Convolutional networks: reducing free parameters and encoding the properties of images \n",
    "- Explore a simple example to understand how filters work\n",
    "\n",
    "\n",
    "### Network Structure\n",
    "\n",
    "Until now we have completely focussed on deep neural networks where all neurons are connected between each layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [9,9]\n",
    "ml4s.draw_network(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have $9\\times 9 + 9 = 90$ parameters.  If we have consider the MNIST data set ($28\\times28 = 784$) with an input layer, 2 hidden layers each with 100 neurons, and a softmax layer we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number weights = {784*100*100*10:g}')\n",
    "print(f'Number biases = {100+100+10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a convolutional layer, we appeal to the action of the kernel's above, they act locally, everywhere across the image.   For a filter (kernel) size of $F=3$ the equivalent convolutional layer would have the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [9,9]\n",
    "\n",
    "# Generate F random weights and set the rest to zero\n",
    "F = 3\n",
    "weights = np.zeros(n)\n",
    "fw = np.random.uniform(low=0.5,high=1.0,size=F)\n",
    "for i in range(n[0]):\n",
    "    for μ in range((F//2)*(-1),(F//2)+1):\n",
    "        if 0 <= i+μ < n[1]:\n",
    "            weights[i,i+μ] = fw[μ+1]\n",
    "            \n",
    "w = [weights]\n",
    "b = [np.random.random()*np.ones(n[1])]\n",
    "\n",
    "ml4s.draw_network(n,weights=w,biases=b, weight_thickness=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we use the **same weights and biases** for each neuron in the connected layer.  Here we have chosen random weights, but for our ConvNet we will scan the weights and biases over the input image (with a step size `stride`, usually 1) and learn the kernel weights and biases via backpropagation!\n",
    "\n",
    "#### Plot the weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(weights, cmap='coolwarm')\n",
    "plt.colorbar(label='weights', shrink=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's consider a 1D input with $5$ neurons with zero padding $P=1$, a stride of $S=1$ and kernel with $F=3$.  We  will take the weights $w = [2,1,-1]$ and biases $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = [7,5]\n",
    "weights = np.zeros(n)\n",
    "fw = [2,1,-1]\n",
    "\n",
    "for i in range(5):\n",
    "    weights[i,i] = fw[0]\n",
    "    \n",
    "for i in range(1,6):\n",
    "    weights[i,i-1] = fw[1]\n",
    "    \n",
    "for i in range(2,7):\n",
    "    weights[i,i-2] = fw[2]\n",
    "\n",
    "w = [weights]\n",
    "b = [np.zeros(n[1])]\n",
    "\n",
    "ml4s.draw_network(n,weights=w,biases=b, weight_thickness=True, node_labels=[[0,1,2,-1,1,-3,0],[-1,5,2,2,-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "\n",
    "1. This process exploits translational invariance, as the filter will pick up a feature located anywhere in the image.\n",
    "2. There is a drastic reduction in the number of weights and biases that need to be stored and learned.  We go from $n_0 \\times n_1$ to the kernel size ($F\\times F$ for 2D images). This is independent of the image resolution (size).\n",
    "3. ConvNets will train faster and are the state-of-the art for image classification tasks.\n",
    "\n",
    "There are some nice lecture notes on CNNs here: https://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "The final hyper-parameter is that we can have multiple filters for each input.  These are called `filters`.  They could represent many operations, e.g. smoothing, extracting edges, etc.  We will have a new set of weights for each channel.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
