{"cells":[{"cell_type":"markdown","metadata":{"id":"8vUhM0FflDVa"},"source":["# Physics 494/594\n","## Building a Feed Forward Neural Network\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRpery1VlDVb"},"outputs":[],"source":["# %load ./include/header.py\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sys\n","from tqdm import trange,tqdm\n","sys.path.append('./include')\n","import ml4s\n","%matplotlib inline\n","%config InlineBackend.figure_format = 'svg'\n","#plt.style.use('./include/notebook.mplstyle')\n","np.set_printoptions(linewidth=120)\n","#ml4s.set_css_style('./include/bootstrap.css')\n","colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"]},{"cell_type":"markdown","metadata":{"id":"drHiYJ9ClDVc"},"source":["## Last Time\n","\n","### [Notebook Link: 02_NN_Structure_Feed_Forward.ipynb](./02_NN_Structure_Feed_Forward.ipynb)\n","\n","- Built our first neural network\n","- randomly assigned weights and biases\n","- performed activiations one layer at a time\n","\n","## Today\n","\n","- Write code to propagate activations through layers\n","- Manually 'train' to discern features"]},{"cell_type":"markdown","metadata":{"id":"PtiWSoOJlDVc"},"source":["### Recall our 3x3 picture\n","\n","I've defined a function `print_rectangle(...)` that will allows for code resuse.  This is a great programming practice!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8N_GfixelDVc"},"outputs":[],"source":["L = 3\n","N0 = L*L\n","x = [0,0,0,1,1,0,1,1,0]\n","\n","def print_rectangle(x):\n","    print(''.join([ci if (i+1)%L else ci+'\\n' for i,ci in\n","                 enumerate([' ▉ ' if cx else ' ░ ' for i,cx in enumerate(x)])]))\n","print_rectangle(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jQXay_JqlDVd"},"outputs":[],"source":["def print_rectangle_1(x):\n","    L = int(np.sqrt(len(x)))\n","    print(*[''.join(i) for i in np.array([' ▉ ',' ░ '])[x].reshape(L,L)],sep='\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AfrsdRI_lDVd"},"outputs":[],"source":["g = np.array([[1,1,1],[0,0,0],[2,2,2]])\n","np.sum(g,axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZfB4vUZnlDVd"},"outputs":[],"source":["print_rectangle(x)"]},{"cell_type":"markdown","metadata":{"id":"2y31otv8lDVd"},"source":["\n","## Feed Forward\n","\n","Previously we manually propagated activations through a deep neural network one layer at a time.\n","\n","Recall, that for a single layer:\n","\\begin{align}\n","a_j^\\ell &= \\sigma\\left(\\boldsymbol{z}^{\\ell}\\right) \\\\\n","&= \\sigma \\left(\\sum_k w_{jk}^\\ell a_k^{\\ell-1} + b_j^\\ell \\right) \\\\\n"," &= \\sigma\\left(\\boldsymbol{\\mathsf{w}}^\\ell \\cdot \\boldsymbol{a}^{\\ell-1} + \\boldsymbol{b}^\\ell\\right)\n","\\end{align}\n","\n","Given the values in the input layer $\\boldsymbol{x} \\equiv \\boldsymbol{a}^0$, and all weights and biases, we want to compute $\\boldsymbol{z}^{\\ell}$, apply the activations sequentially to each layer, and return the output of the entire network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7aIr0zXGlDVd"},"outputs":[],"source":["def feed_forward(a0,w,b):\n","    ''' Compute the output of a deep neural network given the input (a0)\n","        and the weights (w) and biaes (b).\n","    '''\n","    a = []\n","    return a"]},{"cell_type":"markdown","metadata":{"id":"BvLJvX9IlDVd"},"source":["Next, we will randomly set all the weights and biases for the 1 hidden and 1 output layer of our network.  We used a hidden-layer with only 2 neurons, feel free to change this when  you are working on your notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBiP29ijlDVd"},"outputs":[],"source":["N = [9,2,1]\n","w,b = [],[]\n","\n","# append to the weights and biases list.  Make sure you get the dimensions correct!\n","for ℓ in range(1,len(N)):\n","    pass"]},{"cell_type":"markdown","metadata":{"id":"Szil4sCDlDVd"},"source":["Let's compute (and output) the activation of the output layer.\n","\n","We can keep randomly generating new weights and biases (by executing the code above) until we find a set that is close to 1 (which we want for our rectangle)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFfvnvP_lDVe"},"outputs":[],"source":["feed_forward(x,w,b)"]},{"cell_type":"markdown","metadata":{"id":"cQO6r51JlDVe"},"source":["### Visualize the Final Network:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zJCtSz6hlDVe"},"outputs":[],"source":["ml4s.draw_network(N, weights=w, biases=b, node_labels=[])"]},{"cell_type":"markdown","metadata":{"id":"ZCgR8V4GlDVe"},"source":["<div class=\"span alert alert-success\">\n","<h4> Excercises </h4>\n","<ol>\n","    <li>Find the output from the neural network for the following inputs\n","        <p>\n","            <code>x = [1,1,1,0,0,0,0,0,0]</code> <br />\n","            <code>x = [1,0,0,0,1,0,0,0,1]</code> <br />\n","            <code>x = [0,0,0,0,0,0,0,0,1]</code> <br />\n","        </p>\n","       You can use the <code>print_rectangle(x)</code> function to visualize.\n","    </li>\n","    <li> Modify your <code>feed_forward</code> function to use a ReLU instead of a sigmoid.  Are there any changes?\n","    </li>\n","</ol>\n","</div>"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}