{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Physics 494/594\n",
    "\n",
    "# Unsupervised Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./include/header.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from tqdm import trange,tqdm\n",
    "sys.path.append('./include')\n",
    "import ml4s\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "plt.style.use('./include/notebook.mplstyle')\n",
    "np.set_printoptions(linewidth=120)\n",
    "ml4s.set_css_style('./include/bootstrap.css')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "π = np.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Until Now\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "- We have considered learning tasks (regression, classification) where there was labelled data (i.e MNIST, Ising Model, etc.) and we could train a neural network to \"learn\" these features of the data. \n",
    "- Incredibly useful, but many problems in the physical sciences aren't necessarily about prediciton. We usually want to **learn** something about the underlying distribution that generated the observation. \n",
    "- It is not useful in physics to make good predictions with the wrong model. \n",
    "\n",
    "## Today\n",
    "\n",
    "- Our first foray into unsupervised learning, a large and exciting field.\n",
    "- Concerned with discovering structure in unalabelled data.\n",
    "- We will begin with dimensional reduction and clustering/visualization.\n",
    "\n",
    "## Dimensional Reduction\n",
    "\n",
    "Most of our data sets live in a very high dimension (Ising model configurations were $30x30$ so they live in 900 dimensional space!)  Our goal will be to project (or embed) these observations into a lower dimensional space.   This subspace is called the **latent space**.\n",
    "\n",
    "Let's consider a very simple example of points in two spatial dimensions.\n",
    "\n",
    "<!--\n",
    "N = 2000\n",
    "x = np.random.normal(loc=[0,0],scale=[1,0.4], size=[N,2])\n",
    "\n",
    "θ = np.radians(35)\n",
    "R = np.array([[np.cos(θ), -np.sin(θ)], [np.sin(θ), np.sin(θ)]])\n",
    "for i in range(N):\n",
    "    x[i] = R @ x[i]\n",
    "np.savetxt('../data/scatter_2d_pca.dat',x)\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.loadtxt('../data/scatter_2d_pca.dat')\n",
    "plt.scatter(x[:,0],x[:,1], s=1)\n",
    "plt.axis('equal')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "The goal of PCA is to identify the directions of largest variance with the intuition that these correspond to **signal**, while any orthogonal spread in the data is due to **noise**.  It has an equivalent definition in terms of the linear projection that minimizes the average projection cost (mean squared deviation between original vector and its deviation).\n",
    "\n",
    "\n",
    "Consider our usual set of observations $\\{ \\boldsymbol{x}^{(n)} \\}_{n=0}^{N}$ where now we do not have the associated targers or *labels* $y^{(n)}$.  Each $\\boldsymbol{x}^{(n)} \\in \\mathbb{R}^D$ lives in a D-dimensional feature space.  Without loss of generality, we assume that the mean of the data set is zero: \n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\boldsymbol{x} \\rangle = \\frac{1}{N} \\sum_{n=1}^{N} \\boldsymbol{x}^{(n)} = (0,\\dots,0).\n",
    "\\end{equation}\n",
    "\n",
    "If this is not the case, we can simply subtract the mean from each data point.  Our goal is to project the data onto a latent space having dimensionality $M < D$.\n",
    "\n",
    "To begin, let's project onto a 1-dimenisional space, i.e. $M=1$.  The direction of this dimension (i.e. its unit vector) can be described by a single vector, $\\boldsymbol{v}_1$, which we assume has unit norm: $\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_1 = 1$.  We then project each data point, $\\boldsymbol{x}^{(n)}$ onto $\\boldsymbol{v}_1$ via $\\boldsymbol{v}_1^\\top \\boldsymbol{x}^{(n)}$. The variance of the resulting data set is given by:\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^2 \\equiv \\bigg \\langle \\big\\lvert\\boldsymbol{v}_1^{\\top} \\boldsymbol{x}^{(n)} - \\boldsymbol{v}_1^{\\top}\\underbrace{\\langle  \\boldsymbol{x} \\rangle}_{0} \\big\\rvert^2 \\bigg \\rangle  & = \\frac{1}{N-1} \\sum_{n=1}^{N} \\boldsymbol{v}_1^{\\top}  \\boldsymbol{x}^{(n)}  \\boldsymbol{x}^{(n)\\top}  \\boldsymbol{v}_1 \\\\\n",
    "& = \\boldsymbol{v}_1^{\\top} \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1\n",
    "\\end{align}\n",
    "\n",
    "where we have defined\n",
    "\n",
    "\\begin{equation}\n",
    "\\Sigma(\\mathbf{X}) = \\frac{1}{N-1} \\mathbf{X}^{\\top}\\mathbf{X}\n",
    "\\end{equation}\n",
    "\n",
    "to be the $D \\times D$ covariance matrix of the data design matrix: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\left( \\begin{array}{cccc}\n",
    "        x_{1}^{(1)} & x_{2}^{(1)} & \\cdots & x_{D}^{(1)} \\\\\n",
    "\\vdots        &      \\vdots    & \\ddots & \\vdots \\\\\n",
    "        x_{1}^{(N)} & x_{2}^{(N)} & \\cdots & x_{D}^{(N)} \\\\\n",
    "\\end{array}\n",
    "\\right)\\, .\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "We now want to maximize the variance subject to the constraint $\\boldsymbol{v}_1 \\cdot \\boldsymbol{v}_1 = 1$ which we can do by adding a Lagrange multiplier $\\lambda_1$:\n",
    "\n",
    "\\begin{align}\n",
    "%\\frac{\\partial}{\\partial \\boldsymbol{v}_1^{\\top}} \n",
    "\\boldsymbol{\\nabla}_{\\boldsymbol{v}_1^{\\top}} \\left[\\boldsymbol{v}_1^{\\top} \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1 + \\lambda_1 (1-\\boldsymbol{v}_1^{\\top} \\cdot \\boldsymbol{v}_1) \\right] & = \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1 - \\lambda_1 \\boldsymbol{v}_1 = 0 \\\\\n",
    "&\\Rightarrow  \\Sigma(\\mathbf{X})\\boldsymbol{v}_1 = \\lambda_1 \\boldsymbol{v}_1\n",
    "\\end{align}\n",
    "\n",
    "which tells us that $\\boldsymbol{v}_1$ is a left eigenvector of $\\Sigma(\\mathbf{X})$.  If we multiple on the left by $\\boldsymbol{v}_1^{\\top}$ and utilize the unit norm we find:\n",
    "\n",
    "\\begin{equation}\n",
    "\\boxed{\\lambda_1 = \\boldsymbol{v}_1^{\\top} \\Sigma(\\mathbf{X}) \\boldsymbol{v}_1.}\n",
    "\\end{equation}\n",
    "\n",
    "Thus the variance will be maximized when $\\boldsymbol{v}_1$ is chosen to be the the eigenvector with largest eignvalue $\\lambda_1$.  It is known as the **first principle component**.  Not surprisingly, we can determine additional principal components in an iterative fashion which maximizes the projected variance amongst all possible directions orthogonal to the ones already considered.  This identifies the $M$ principal components as the $M$ eigenvectors corresponding to the $M$ largest eigenvalues $\\boldsymbol{v}_j$ of $\\Sigma(\\mathbf{X})$.\n",
    "\n",
    "In practice, we simply diagonlize the symmetrix square matrix $\\Sigma(\\mathbf{X}) = \\mathbf{X}^{\\top}\\mathbf{X}/(N-1)$, the principle components are the eigenvalues $\\lambda_j$ and we organize the eigenvectors into a column matrix $\\boldsymbol{V}$.\n",
    "\n",
    "We can do this with the `scipy.linalg` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "N = x.shape[0]\n",
    "x -= np.average(x,axis=0)\n",
    "Σ = x.T @ x / (N-1)\n",
    "λ,V = scipy.linalg.eigh(Σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "    Note: <code>eigh()</code> returns eigenvalues in ascending order!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = λ[::-1]\n",
    "V = np.flip(V,axis=1)\n",
    "\n",
    "print(f'λ = {λ}')\n",
    "print(f'V = {V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important quantity is the *percentage of the explained variance* defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{PCA-j} = \\frac{\\lambda_j}{\\sum_{j=1}^{D} \\lambda_j}\n",
    "\\end{equation}\n",
    "\n",
    "We can plot the resulting axes defined by the principal components, or alternatively, can define a projection operator (matrix):\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{P} = \\sum_{j=1}^M\\boldsymbol{v}_j\\boldsymbol{v}_j^\\mathsf{T}\n",
    "\\end{equation}\n",
    "\n",
    "to plot with respect to the new axes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "ax[0].scatter(x[:,0],x[:,1], s=1, alpha=0.5)\n",
    "\n",
    "_x = np.linspace(-4,4,100)\n",
    "_y = np.linspace(-0.5,0.5,100)\n",
    "ax[0].plot(_x,V[1,0]/V[0,0]*_x, '-', color=colors[0], label=f'PCA-1 = {λ[0]/np.sum(λ):.2f}')\n",
    "ax[0].plot(_y,V[1,1]/V[0,1]*_y, '-', color=colors[-2], label=f'PCA-2 = {λ[1]/np.sum(λ):.2f}')\n",
    "\n",
    "ax[0].axis('equal')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].legend()\n",
    "\n",
    "# perform the projection\n",
    "px = x @ V\n",
    "\n",
    "# this can alternatively be down via direct projection\n",
    "direct = False\n",
    "if direct:\n",
    "    px = np.zeros_like(x)\n",
    "    for n in range(x.shape[0]):\n",
    "        for j in range(2):\n",
    "            px[n,j] = np.dot(V[:,j],x[n,:])\n",
    "    \n",
    "ax[1].scatter(px[:,0],px[:,1], s=1, alpha=0.5, label=r'$\\mathbf{X} \\mathbf{V}$')\n",
    "ax[1].set_xlabel(f'PCA-1 = {λ[0]/np.sum(λ):.2f}')\n",
    "ax[1].set_ylabel(f'PCA-2 = {λ[1]/np.sum(λ):.2f}')\n",
    "ax[1].axis('equal')\n",
    "ax[1].legend()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKLearn Implementation\n",
    "\n",
    "Obviously this is useful enough that we don't have to code it up from scratch every time we want the principal components.  It has a convenient implementation in `sklearn`.\n",
    "\n",
    "Our first step is to scale the data (so-called standard or z-scaling) such that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z} = \\frac{\\mathbf{x}-\\langle \\mathbf{x} \\rangle}{\\sqrt{\\langle \\mathbf{x}^{\\top}\\mathbf{x} \\rangle - \\lvert \\langle \\mathbf{x} \\rangle \\rvert^2}} \\, .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_x = scaler.fit_transform(x)\n",
    "\n",
    "model = PCA(n_components=2)\n",
    "XPCA = model.fit_transform(scaled_x)\n",
    "\n",
    "λ = model.explained_variance_\n",
    "V = model.components_.T\n",
    "PCAj = model.explained_variance_ratio_\n",
    "\n",
    "print(f'λ = {λ}')\n",
    "print(f'V = {V}')\n",
    "print(f'PCA-j = {PCAj}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"span alert alert-warning\">\n",
    "    Note: To be extra confusing, <code>model.components_.</code> returns the principal vectors as rows! I take the transpose above to compare with our eigenvectors above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(8,4))\n",
    "\n",
    "ax[0].scatter(scaled_x[:,0],scaled_x[:,1], s=1, alpha=0.5)\n",
    "\n",
    "_x = np.linspace(-4,4,100)\n",
    "_y = np.linspace(-0.5,0.5,100)\n",
    "ax[0].plot(_x,V[1,0]/V[0,0]*_x, '-', color=colors[0], label=f'PCA-1 = {λ[0]/np.sum(λ):.2f}')\n",
    "ax[0].plot(_y,V[1,1]/V[0,1]*_y, '-', color=colors[-2], label=f'PCA-2 = {λ[1]/np.sum(λ):.2f}')\n",
    "\n",
    "ax[0].axis('equal')\n",
    "#ax[0].set_xticks([])\n",
    "#ax[0].set_yticks([])\n",
    "ax[0].legend()\n",
    "\n",
    "# perform the projection\n",
    "px = scaled_x @ V\n",
    "    \n",
    "ax[1].scatter(px[:,0],px[:,1], s=1, alpha=0.5, label=r'$\\mathbf{X} \\mathbf{V}$')\n",
    "ax[1].set_xlabel(f'PCA-1 = {λ[0]/np.sum(λ):.2f}')\n",
    "ax[1].set_ylabel(f'PCA-2 = {λ[1]/np.sum(λ):.2f}')\n",
    "ax[1].axis('equal')\n",
    "ax[1].legend()\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
